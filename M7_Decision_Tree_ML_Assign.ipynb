{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work ?\n",
        "\n",
        "\n",
        "A decision tree is a flowchart-like structure used in machine learning and decision analysis to predict outcomes or make decisions by recursively splitting data based on attributes. It works by starting with a root node, branching out based on conditions (internal nodes), and ultimately reaching leaf nodes representing the final prediction or decision."
      ],
      "metadata": {
        "id": "n_h-mCFQaFSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees ?\n",
        "\n",
        "\n",
        "In decision trees, impurity measures quantify how \"mixed\" the classes are within a node, with common metrics including Gini impurity and entropy, used to guide splitting decisions and build the tree."
      ],
      "metadata": {
        "id": "30k27q_jaFjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. What is the mathematical formula for Gini Impurity ?\n",
        "\n",
        "\n",
        " The mathematical formula for Gini Impurity, used to measure the impurity of a set of data in decision trees, is 1 - Σ (p<sub>i</sub><sup>2</sup>), where p<sub>i</sub> represents the probability of an instance belonging to class i."
      ],
      "metadata": {
        "id": "kkAoywvVaFs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the mathematical formula for Entropy ?\n",
        "\n",
        "\n",
        "\n",
        "In one statistical interpretation of entropy, it is found that for a very large system in thermodynamic equilibrium, entropy S is proportional to the natural logarithm of a quantity Ω representing the maximum number of microscopic ways in which the macroscopic state corresponding to S can be realized; that is, S = k ln Ω, in which k is the Boltzmann constant that is related to molecular energy."
      ],
      "metadata": {
        "id": "gLkzlGZWaFwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Information Gain, and how is it used in Decision Trees ?\n",
        "\n",
        "\n",
        "Information Gain measures the reduction in entropy (uncertainty) of a dataset after splitting it based on a particular feature, and it's used in decision trees to determine the best feature to split on at each node."
      ],
      "metadata": {
        "id": "_Fy6ikoKaF2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Gini Impurity:\n",
        "Definition:\n",
        "Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the class distribution in the dataset.\n",
        "Calculation:\n",
        "It's calculated as 1 minus the sum of the squared probabilities of each class.\n",
        "Range:\n",
        "Gini impurity ranges from 0 to 0.5, with 0 representing a pure node (all data points belong to the same class) and 0.5 representing the highest impurity.\n",
        "Computational Complexity:\n",
        "Gini impurity is computationally simpler and faster to calculate than entropy because it involves simple arithmetic operations rather than logarithms.\n",
        "Use in Decision Trees:\n",
        "Gini impurity is used by the CART (Classification and Regression Tree) algorithm for classification trees.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Entropy:\n",
        "Definition: Entropy, rooted in information theory, measures the amount of uncertainty or randomness in a dataset.\n",
        "Calculation: Entropy is calculated as the negative sum of the probabilities of each class multiplied by the logarithm of their probabilities.\n",
        "Range: Entropy ranges from 0 to 1, with 0 representing a pure node and 1 representing the highest impurity.\n",
        "Computational Complexity: Entropy is more computationally intensive because it involves logarithms.\n",
        "Use in Decision Trees: Entropy is used in algorithms like C4."
      ],
      "metadata": {
        "id": "TZiMWL63aF6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees ?\n",
        "\n",
        "\n",
        "Decision trees rely on mathematical concepts like splitting criteria (e.g., Gini impurity, information gain) to determine the best way to partition data, ultimately leading to a tree structure that predicts outcomes."
      ],
      "metadata": {
        "id": "JV4FVEuLaF9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Pre-Pruning in Decision Trees ?\n",
        "\n",
        "Pre-pruning, also known as early stopping, is a technique used in decision trees to prevent overfitting by stopping the tree's growth before it reaches its full potential, limiting its complexity and size during the initial building phase."
      ],
      "metadata": {
        "id": "qQ_tTiWyaGAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Post-Pruning in Decision Trees ?\n",
        "\n",
        "\n",
        "\n",
        "Post-pruning in decision trees involves removing branches or nodes after the tree has been fully grown, aiming to simplify the model and improve its generalization ability by preventing overfitting."
      ],
      "metadata": {
        "id": "HCq_jx1UaGDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between Pre-Pruning and Post-Pruning ?\n",
        "\n",
        "\n",
        "In the context of decision trees, pre-pruning (early stopping) stops tree growth before it's fully developed, while post-pruning (reduced error pruning) removes branches after the tree is fully grown to improve generalization."
      ],
      "metadata": {
        "id": "5aVRl4YSfVxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is a Decision Tree Regressor ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A Decision Tree Regressor is a supervised machine learning algorithm that uses a tree-like model to predict continuous (numeric) target variables by recursively splitting data based on features that best reduce prediction error."
      ],
      "metadata": {
        "id": "0s3QsfhBfhCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Advantages:\n",
        "Interpretability:\n",
        "Decision trees are easy to understand and visualize, making it simpler to explain their predictions.\n",
        "Minimal Data Preparation:\n",
        "They require less data cleaning and preprocessing compared to other algorithms, as they can handle missing values and outliers relatively well.\n",
        "Handles Both Numerical and Categorical Data:\n",
        "Decision trees can effectively process both types of data without requiring separate handling.\n",
        "Feature Selection and Importance:\n",
        "Decision trees can identify and rank the importance of different features, helping to understand which factors are most influential in making predictions.\n",
        "Non-Parametric:\n",
        "They don't make assumptions about the underlying data distribution, making them versatile for various datasets.\n",
        "Robust to Outliers:\n",
        "Decision trees are less influenced by outliers compared to some other algorithms.\n",
        "Can Handle Imbalanced Data:\n",
        "Decision trees can be used for classification tasks with imbalanced datasets, where one class significantly outnumbers the others.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "\n",
        "Overfitting:\n",
        "Decision trees can become overly complex and learn the training data too well, leading to poor generalization on unseen data.\n",
        "High Variance:\n",
        "Small changes in the training data can lead to significant changes in the tree structure and predictions, resulting in high variance.\n",
        "Bias Towards Dominant Classes:\n",
        "In classification tasks with imbalanced datasets, decision trees can be biased towards the dominant class, leading to poor predictive performance for minority classes.\n",
        "Sensitivity to Data Variations:\n",
        "Small changes in the training data can lead to significant changes in the tree structure and predictions.\n",
        "Can Be Computationally Expensive:\n",
        "Building and using large decision trees can be computationally intensive, especially for large datasets.\n",
        "Difficult to Prune Effectively:\n",
        "Finding the optimal size for a pruned tree can be challenging, and improper pruning can lead to underfitting."
      ],
      "metadata": {
        "id": "vVEghOsRfo0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. How does a Decision Tree handle missing values ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Decision trees can handle missing values in several ways, including using surrogate splits, treating missing values as a separate category, or distributing instances with missing values to child nodes."
      ],
      "metadata": {
        "id": "ARGGj9clgMUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does a Decision Tree handle categorical features ?\n",
        "\n",
        "\n",
        "\n",
        "Decision trees can handle categorical features, either through direct processing or by first encoding them numerically using methods like one-hot encoding or label encoding, depending on the algorithm and implementation."
      ],
      "metadata": {
        "id": "ebEqmSp7gXIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Decision trees find applications across diverse fields, including healthcare (diagnosing conditions), finance (loan approval, fraud detection), and business (marketing, customer segmentation), by helping to analyze data, make predictions, and simplify complex decision-making processes.\n",
        "Here's a more detailed look at some real-world applications:"
      ],
      "metadata": {
        "id": "ypELcu9AgtsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      **practical**"
      ],
      "metadata": {
        "id": "YVQWlXrLg5NY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."
      ],
      "metadata": {
        "id": "OrotxLxIhDRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def importdata():\n",
        "    balance_data = pd.read_csv(\n",
        "        'https://archive.ics.uci.edu/ml/machine-learning-' +\n",
        "        'databases/balance-scale/balance-scale.data',\n",
        "        sep=',', header=None)\n",
        "\n",
        "    # Displaying dataset information\n",
        "    print(\"Dataset Length: \", len(balance_data))\n",
        "    print(\"Dataset Shape: \", balance_data.shape)\n",
        "    print(\"Dataset: \", balance_data.head())\n",
        "\n",
        "    return balance_data\n"
      ],
      "metadata": {
        "id": "KpSbperVhL7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "#feature importances\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Simulating some data\n",
        "n = 1000\n",
        "\n",
        "shares_y1 = [\n",
        "    0,\n",
        "    0.1,\n",
        "    0.2,\n",
        "    0.3,\n",
        "    0.4,\n",
        "    0.5,\n",
        "    0.6,\n",
        "    0.7,\n",
        "    0.8,\n",
        "    0.9,\n",
        "    1\n",
        "]\n",
        "\n",
        "y1_counts = [x * n for x in shares_y1]\n",
        "y2_counts = [n - x for x in y1_counts]\n",
        "\n",
        "y = list(zip(y1_counts, y2_counts))\n",
        "\n",
        "\n",
        "\n",
        "y\n",
        "\n",
        "\n",
        "\n",
        "# Getting the GINI impurities for such data\n",
        "ginis = [Node.GINI_impurity(x[0], x[1]) for x in\n",
        "         y]\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(shares_y1, ginis, '-o')\n",
        "plt.xlabel(\"Share of first class in the whole dataset\")\n",
        "plt.ylabel(\"GINI impurity\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "collapsed": true,
        "id": "GSQAOqy7ivfU",
        "outputId": "cddc66c8-5859-40c7-edf7-23046658ef27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Node' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fa071d44a89f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Getting the GINI impurities for such data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m ginis = [Node.GINI_impurity(x[0], x[1]) for x in \n\u001b[0m\u001b[1;32m     37\u001b[0m          y]\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-fa071d44a89f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Getting the GINI impurities for such data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m ginis = [Node.GINI_impurity(x[0], x[1]) for x in \n\u001b[0m\u001b[1;32m     37\u001b[0m          y]\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "#model accuracy.\n",
        "\n",
        "\n",
        "\n",
        "def train_using_entropy(X_train, X_test, y_train):\n",
        "\n",
        "    # Decision tree with entropy\n",
        "    clf_entropy = DecisionTreeClassifier(\n",
        "        criterion=\"entropy\", random_state=100,\n",
        "        max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "    # Performing training\n",
        "    clf_entropy.fit(X_train, y_train)\n",
        "    return clf_entropy"
      ],
      "metadata": {
        "id": "ywKzx7AHj8_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "#Squared Error (MSE).\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Given values\n",
        "Y_true = [1,1,2,2,4]  # Y_true = Y (original values)\n",
        "\n",
        "# calculated values\n",
        "Y_pred = [0.6,1.29,1.99,2.69,3.4]  # Y_pred = Y'\n",
        "\n",
        "# Calculation of Mean Squared Error (MSE)\n",
        "mean_squared_error(Y_true,Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXccpUstkKIp",
        "outputId": "1494f8e9-3a17-4bf5-84af-92e729924124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21606"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20.Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "df.head()\n",
        "\n",
        "tree.export_graphviz(clf,\n",
        "                     out_file=\"tree.dot\",\n",
        "                     feature_names = fn,\n",
        "                     class_names=cn,\n",
        "                     filled = True)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "tree.export_graphviz(clf,\n",
        "                     out_file=\"treeRotated.dot\",\n",
        "                     feature_names = fn,\n",
        "                     class_names=cn,\n",
        "                     rotate = True,\n",
        "                     filled = True)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9TNEck3olIv4",
        "outputId": "3c523c64-1034-4083-c339-eca1e86bd9b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1000),\n",
              " (100.0, 900.0),\n",
              " (200.0, 800.0),\n",
              " (300.0, 700.0),\n",
              " (400.0, 600.0),\n",
              " (500.0, 500.0),\n",
              " (600.0, 400.0),\n",
              " (700.0, 300.0),\n",
              " (800.0, 200.0),\n",
              " (900.0, 100.0),\n",
              " (1000, 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "#accuracy with a fully grown tree.\n",
        "\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(scores[\"param_max_depth\"],\n",
        "         scores[\"mean_train_score\"],\n",
        "         label=\"training accuracy\")\n",
        "plt.plot(scores[\"param_max_depth\"],\n",
        "         scores[\"mean_test_score\"],\n",
        "         label=\"test accuracy\")\n",
        "plt.xlabel(\"max_depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wiMj6CbEneQL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "e84f165a-355d-42a0-bd46-0405789608d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'scores' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f657a2231114>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m plt.plot(scores[\"param_max_depth\"], \n\u001b[0m\u001b[1;32m      8\u001b[0m          \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean_train_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m          label=\"training accuracy\")\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iXE9ZbevsTMc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SVH5r5tsV7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "#accuracy with a default tree.\n",
        "\n",
        "\n",
        "# GridSearchCV to find optimal max_depth\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# specify number of folds for k-fold CV\n",
        "n_folds = 5\n",
        "\n",
        "# parameters to build the model on\n",
        "parameters = {'min_samples_leaf': range(1, 40, 3)}\n",
        "\n",
        "# instantiate the model\n",
        "dtree = DecisionTreeClassifier(criterion = \"gini\",\n",
        "                               random_state = 100)\n",
        "\n",
        "# fit tree on training data\n",
        "tree = GridSearchCV(dtree, parameters,\n",
        "                    cv=n_folds,\n",
        "                   scoring=\"accuracy\", return_train_score=True)\n",
        "tree.fit(X_train, y_train)\n",
        "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=100),\n",
        "             param_grid={'min_samples_leaf': range(1, 40, 3)},\n",
        "             return_train_score=True, scoring='accuracy')\n",
        "# scores of GridSearch CV\n",
        "scores = tree.cv_results_\n",
        "pd.DataFrame(scores).head()"
      ],
      "metadata": {
        "id": "iMJwOyEnlPaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23.Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "#accuracy with unscaled data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "iristree1 = DecisionTreeClassifier(criterion = \"entropy\",\n",
        "                                  random_state = 100,\n",
        "                                  max_depth=4,\n",
        "                                  min_samples_leaf=3,\n",
        "                                  min_samples_split=2)\n",
        "iristree1.fit(X_train, y_train)\n",
        "DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=3,\n",
        "                       random_state=100)\n",
        "# accuracy score\n",
        "iristree1.score(X_test,y_test)\n",
        "\n",
        "# plotting the tree\n",
        "dot_data = StringIO()\n",
        "export_graphviz(iristree1, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
        "\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "HUsydyHasYkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Fit the model\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fig(X,y)\n",
        "#Generate predictions with the model using our X values\n",
        "y_pred = logreg.predict(X)\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(y, y_pred)\n",
        "print(cf_matrix)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cf_matrix, annot=True)"
      ],
      "metadata": {
        "id": "TMiBCVEEvf71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "#for max_depth and min_samples_split.\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Create the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Set up the GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_searc.best_score_)"
      ],
      "metadata": {
        "id": "eXniB3KkwJ71"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}