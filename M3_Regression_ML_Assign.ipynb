{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression ?\n",
        "  \n",
        "\n",
        "  Simple linear regression aims to find a linear relationship to describe the correlation between an independent and possibly dependent variable. The regression line can be used to predict or estimate missing values, this is known as interpolation."
      ],
      "metadata": {
        "id": "FiI1YtJ8DyQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  What are the key assumptions of Simple Linear Regression ?\n",
        "\n",
        "\n",
        "\n",
        "   The key assumptions of Simple Linear Regression are: a linear relationship between the variables, independence of errors, homoscedasticity (constant variance of errors), and normality of the error distribution."
      ],
      "metadata": {
        "id": "ef5vm1ONEPGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "\n",
        "   In the equation Y = mX + c, the coefficient 'm' represents the slope or gradient of the line, indicating how steeply the line rises or falls.\n",
        "Here's a more detailed explanation:\n",
        "Slope/Gradient:\n",
        "The value of 'm' determines the steepness (or inclination) of the line. A positive 'm' means the line slopes upwards from left to right, while a negative 'm' indicates a downward slope.\n",
        "'c' (y-intercept):\n",
        "The constant 'c' represents the y-coordinate where the line intersects the y-axis.\n",
        "Equation:\n",
        "The equation Y = mX + c is known as the slope-intercept form of a linear equation.\n",
        "Example:\n",
        "If m = 2 and c = 1, the equation would be Y = 2X + 1, representing a line with a slope of 2 that crosses the y-axis at the point (0, 1)."
      ],
      "metadata": {
        "id": "H3swiBFiEe9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What does the intercept c represent in the equation Y=mX+c ?\n",
        "\n",
        "   The general equation of a straight line is y = mx + c, where m is the gradient, and y = c is the value where the line cuts the y-axis. This number c is called the intercept on the y-axis. The equation of a straight line with gradient m and intercept c on the y-axis is y = mx + c."
      ],
      "metadata": {
        "id": "aAxhSVEnFyvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How do we calculate the slope m in Simple Linear Regression ?\n",
        "\n",
        "\n",
        "   In simple linear regression, the slope (m) is calculated by dividing the covariance of X and Y by the variance of X, represented as: m = Cov(X,Y) / Var(X).\n",
        "Here's a more detailed explanation:\n",
        "Understanding the Formula:\n",
        "m: Represents the slope of the regression line, indicating the change in the dependent variable (Y) for every unit change in the independent variable (X).\n",
        "Cov(X, Y): Represents the covariance between the independent variable (X) and the dependent variable (Y). It measures how much the two variables change together.\n",
        "Var(X): Represents the variance of the independent variable (X). It measures how spread out the X values are."
      ],
      "metadata": {
        "id": "_w5QPWoqF-Ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the purpose of the least squares method in Simple Linear Regression ?  \n",
        "\n",
        "\n",
        "   The least squares method in simple linear regression aims to find the \"best-fit\" line (regression line) by minimizing the sum of the squared differences (residuals) between the actual data points and the predicted values on the line.\n",
        "  "
      ],
      "metadata": {
        "id": "jaAHYK3FGPA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "\n",
        "\n",
        "  In simple linear regression, the coefficient of determination (R²) represents the proportion of the variance in the dependent variable that is explained by the independent variable, ranging from 0 to 1, with higher values indicating a better model fit."
      ],
      "metadata": {
        "id": "KQlImTwjV5MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Multiple Linear Regression ?\n",
        "\n",
        "\n",
        "   Multiple linear regression is a statistical technique that uses two or more independent variables to predict the outcome of a single dependent variable, allowing analysts to determine the variation of the model and the relative contribution of each independent variable."
      ],
      "metadata": {
        "id": "USZwXRvKWGpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "\n",
        "\n",
        "   Simple Linear Regression:\n",
        "\n",
        "Models the relationship between one independent variable (predictor) and one dependent variable (response).\n",
        "The goal is to find a linear equation that best fits the data points and minimizes the error between the actual and predicted values.\n",
        "Uses a single slope to represent the relationship.\n",
        "\n",
        "Example: Predicting house prices based on square footage.\n",
        "\n",
        "\n",
        "Multiple Linear Regression:\n",
        "\n",
        "\n",
        "Models the relationship between one dependent variable and two or more independent variables.\n",
        "The goal is to find a linear equation that best fits the data points and minimizes the error between the actual and predicted values, considering the influence of multiple predictors.\n",
        "Uses multiple slopes, one for each independent variable, to represent the relationship.\n",
        "\n",
        "\n",
        "Example: Predicting house prices based on square footage, number of bedrooms, and location."
      ],
      "metadata": {
        "id": "yS9L3smeWVn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "\n",
        "\n",
        "\n",
        "The key assumptions of Multiple Linear Regression are linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of residuals."
      ],
      "metadata": {
        "id": "N0FmvSesW65R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression mode ?\n",
        "\n",
        "Heteroscedasticity, in the context of multiple linear regression, refers to the situation where the variance of the error terms (residuals) is not constant across all levels of the independent variables, violating a key assumption of the model. This can lead to unreliable standard errors and inaccurate statistical inferences, even though the coefficient estimates themselves may remain unbiased."
      ],
      "metadata": {
        "id": "QhMuUfG9XIti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "\n",
        "To improve a multiple linear regression model with high multicollinearity, you can remove or combine highly correlated variables, use techniques like Principal Component Analysis (PCA) for dimensionality reduction, or employ regularization methods like Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "wgcCDiwmXZMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "\n",
        "\n",
        "\n",
        "To transform categorical variables for use in regression models, common techniques include one-hot encoding, label encoding, and feature hashing. One-hot encoding creates binary columns for each category, label encoding assigns numerical labels, and feature hashing reduces dimensionality for high-cardinality variables."
      ],
      "metadata": {
        "id": "6Tzk-azOXlRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "\n",
        "\n",
        "\n",
        "n Multiple Linear Regression, interaction terms reveal if the effect of one independent variable on the dependent variable changes depending on the value of another independent variable, indicating a non-additive relationship."
      ],
      "metadata": {
        "id": "mLmzUbJPXw1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "\n",
        "\n",
        "In both simple and multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero. However, in multiple regression, the intercept also accounts for the combined effect of other independent variables, making its interpretation more nuanced."
      ],
      "metadata": {
        "id": "fDJzkKOnX_KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "\n",
        "\n",
        "In regression analysis, the slope signifies the rate of change in the dependent variable for every one-unit change in the independent variable, and it directly impacts predictions by determining the direction and magnitude of the predicted change."
      ],
      "metadata": {
        "id": "bf3CBl44YKl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "\n",
        "\n",
        "The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero. In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero."
      ],
      "metadata": {
        "id": "brZN9urFYUck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "\n",
        "\n",
        " it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-squared isn't necessarily good or bad—it doesn't convey the reliability of the model or whether you've chosen the right regression."
      ],
      "metadata": {
        "id": "cpd39F86Ygmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How would you interpret a large standard error for a regression coefficient ?\n",
        "\n",
        "A large standard error for a regression coefficient suggests that the estimated coefficient is highly variable and less precise, meaning the true population value could be significantly different from the estimated value. This indicates less confidence in the reliability of the coefficient as a predictor."
      ],
      "metadata": {
        "id": "C51T81w4Zl3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "\n",
        "Heteroscedasticity, or unequal variance of residuals, can be identified in residual plots by a \"fan-shaped\" or \"cone-shaped\" pattern where the spread of residuals increases or decreases with the fitted values. Addressing it is crucial because it violates a key assumption of linear regression, leading to unreliable parameter estimates and invalid confidence intervals."
      ],
      "metadata": {
        "id": "6SSTvUyFZw2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "\n",
        "\n",
        "When a multiple linear regression model has a high R-squared but a low adjusted R-squared, it suggests that the model might be overfitting the data by including too many variables that don't significantly improve the predictive power, even though they inflate the R-squared value."
      ],
      "metadata": {
        "id": "dQasTCl0Z-lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "\n",
        " Scaling variables in Multiple Linear Regression is important to ensure a more robust and interpretable model, especially when dealing with variables on different scales, as it helps prevent larger-scale variables from dominating the model and makes it easier to compare the effects of different predictors."
      ],
      "metadata": {
        "id": "Om2aK_bJaIl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression ?\n",
        "\n",
        "Polynomial regression is a form of regression analysis that models the relationship between a dependent variable and one or more independent variables as an nth-degree polynomial function, allowing for capturing non-linear relationships that linear regression cannot."
      ],
      "metadata": {
        "id": "RaR7OMA2aV9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression ?\n",
        "\n",
        "\n",
        "Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. Unlike linear regression, polynomial regression can fit non-linear relationships between variables."
      ],
      "metadata": {
        "id": "SJE1W7WUaf4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used ?\n",
        "\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent and dependent variables is not linear, but rather exhibits a non-linear, curvilinear pattern that can be modeled by a polynomial equation."
      ],
      "metadata": {
        "id": "30FFY-ZDaufC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.  What is the general equation for polynomial regression ?\n",
        "\n",
        "\n",
        "The general equation for polynomial regression, used to model non-linear relationships, is y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε, where 'y' is the dependent variable, 'x' is the independent variable, 'β₀, β₁, β₂, ... βₙ' are the coefficients, 'n' is the degree of the polynomial, and 'ε' is the error term."
      ],
      "metadata": {
        "id": "Q93dMJroa9qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables ?\n",
        "\n",
        "\n",
        "\n",
        "Yes, polynomial regression can be applied to multiple variables, allowing you to model complex, non-linear relationships between multiple independent variables and a dependent variable."
      ],
      "metadata": {
        "id": "GvlvpeVAbJnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.  What are the limitations of polynomial regression ?\n",
        "\n",
        "\n",
        "Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses generalization ability. Careful model selection and regularization techniques are required to mitigate this risk."
      ],
      "metadata": {
        "id": "6FKPElLUbTzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.  What methods can be used to evaluate model fit when selecting the degree of a polynomia ?\n",
        "\n",
        "\n",
        "To evaluate model fit and select the appropriate degree for a polynomial, you can use methods like visual inspection of plots, cross-validation techniques, and model evaluation metrics such as R-squared, adjusted R-squared, or Root Mean Squared Error (RMSE)."
      ],
      "metadata": {
        "id": "zITnegpPbfvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression ?\n",
        "\n",
        "Visualization is crucial in polynomial regression to understand how well the model fits the data, identify potential overfitting or underfitting, and assess the nature of the relationship between variables, especially when dealing with non-linear trends."
      ],
      "metadata": {
        "id": "NftdUYiCbrB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "\n",
        "Step 1: Import the required python packages. ...\n",
        "Step 2: Load the dataset. ...\n",
        "Step 3: Data analysis. ...\n",
        "Step 4: Split the dataset into dependent/independent variables. ...\n",
        "Step 5: Train the regression model. ...\n",
        "Step 6: Predict the result."
      ],
      "metadata": {
        "id": "Vhl15CIMb2ed"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}