{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression ?\n",
        "\n",
        "\n",
        "     Logistic Regression is a statistical method used for binary classification, meaning it predicts the probability of an instance belonging to a specific class (e.g., yes/no, spam/not spam). It differs from Linear Regression, which predicts a continuous output value.\n",
        "     Linear Regression is used for regression problems where the dependent variable is continuous. Logistic Regression, on the other hand, is used for classification problems where the dependent variable is categorical.\n",
        "\n"
      ],
      "metadata": {
        "id": "t2WJuA_l2Gq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "  The mathematical equation of Logistic Regression is:\n",
        "\n",
        " p(y=1|x) = 1 / (1 + exp(-z))\n",
        "\n",
        " where:\n",
        "\n",
        " * p(y=1|x) is the probability of the instance belonging to class 1, given its features x.\n",
        "* z = wTx + b is the linear combination of the input features, where w is the weight vector, x is the feature vector, and b is the bias.\n",
        " * exp is the exponential function.  "
      ],
      "metadata": {
        "id": "TZj9sCi24Gw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "  The Sigmoid function maps the linear combination of input features (z) to a probability between 0 and 1. This makes it suitable for binary classification problems where we want to estimate the probability of an instance belonging to a specific class. The sigmoid function's S-shaped curve ensures that the output is always within the range of  [0, 1].\n"
      ],
      "metadata": {
        "id": "AvOYp4aD4ZAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. What is the cost function of Logistic Regression?\n",
        "\n",
        "   The cost function for Logistic Regression is the log loss function, also known as the cross-entropy loss. It measures the difference between the predicted probability and the actual class label.\n",
        "  The goal is to minimize this cost function by adjusting the model's parameters (weights and bias) using an optimization algorithm like gradient descent.\n"
      ],
      "metadata": {
        "id": "IJ2MzBaS4j6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        " Regularization is a technique used to prevent overfitting in Logistic Regression. Overfitting occurs when the model fits the training data too well, resulting in poor performance on unseen data.\n",
        "\n",
        " Regularization techniques add a penalty term to the cost function, encouraging the model to have smaller weights. This helps in preventing the model from becoming too complex and improving its generalization ability.\n",
        "\n"
      ],
      "metadata": {
        "id": "yPoJnVIx4tmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        " Lasso (L1 regularization): Adds the sum of the absolute values of the coefficients as a penalty term to the cost function. It can lead to feature selection by shrinking some coefficients to zero.\n",
        " Ridge (L2 regularization): Adds the sum of the squared values of the coefficients as a penalty term. It helps to reduce the magnitude of coefficients without necessarily setting them to zero.\n",
        " Elastic Net: Combines both L1 and L2 regularization, providing a balance between feature selection and coefficient shrinkage.\n"
      ],
      "metadata": {
        "id": "1Qkv45NX45EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        " Elastic Net should be used when you have a dataset with a high number of features and some of them are highly correlated. Lasso might struggle in such situations, as it tends to select only one variable from a group of correlated variables. Elastic Net can handle correlated features more effectively by shrinking the coefficients of the correlated variables together.\n",
        "\n"
      ],
      "metadata": {
        "id": "ngoyJ8zK5I_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "  The regularization parameter (λ) controls the strength of the regularization penalty. A higher λ value leads to stronger regularization, resulting in smaller coefficients. This helps to prevent overfitting but might also lead to underfitting if λ is too high.\n",
        "\n"
      ],
      "metadata": {
        "id": "2RlPgtlQ5Nlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "   1. The dependent variable should be binary or categorical.\n",
        " 2. The observations should be independent of each other.\n",
        " 3. There should be a linear relationship between the independent variables and the log odds of the dependent variable.\n",
        "   4. There should be no multicollinearity among independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "fdJB5CjQ5T5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        " Some alternatives to Logistic Regression for classification tasks include:\n",
        "\n",
        " Support Vector Machines (SVM)\n",
        " Decision Trees\n",
        " Random Forests\n",
        " K-Nearest Neighbors (KNN)\n",
        " Naive Bayes\n",
        "Neural Networks"
      ],
      "metadata": {
        "id": "vTvnEQFG5fA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What are Classification Evaluation Metrics?\n",
        "\n",
        " Classification evaluation metrics are used to assess the performance of a classification model. Some common metrics include:\n",
        "\n",
        "Accuracy: The proportion of correctly classified instances.\n",
        " Precision: The proportion of true positive predictions among all positive predictions.\n",
        " Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances.\n",
        "F1-score: The harmonic mean of precision and recall. It provides a balanced measure of both precision and recall.\n",
        " ROC AUC (Area Under the Receiver Operating Characteristic Curve): Measures the model's ability to distinguish between classes.\n"
      ],
      "metadata": {
        "id": "1ZkmA0vD5rFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "# Class imbalance occurs when one class has significantly more instances than the other. This can affect Logistic Regression in several ways:\n",
        "\n",
        "# * The model may be biased towards the majority class, leading to poor performance on the minority class.\n",
        "# * The model may achieve high accuracy but perform poorly on the class of interest.\n",
        "# * The evaluation metrics like accuracy may be misleading, as the model can achieve high accuracy by simply predicting the majority class.\n"
      ],
      "metadata": {
        "id": "IPVSKHzr5y0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "# Hyperparameter tuning involves finding the optimal values for the model's hyperparameters, which are settings that are not learned from the data but are set before training.\n",
        "# In Logistic Regression, some common hyperparameters include:\n",
        "\n",
        "# * Regularization parameter (λ): Controls the strength of regularization.\n",
        "# * Solver: The optimization algorithm used to find the model's parameters.\n",
        "# * Maximum iterations: The maximum number of iterations for the optimization algorithm.\n",
        "# * Tolerance: The tolerance for convergence of the optimization algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "5f6Tx4sn6Cnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "# Different solvers are available in Logistic Regression for finding the optimal model parameters. Some common solvers include:\n",
        "\n",
        "# * Liblinear: Suitable for small datasets and binary classification problems.\n",
        "# * LBFGS: Suitable for larger datasets and multiclass classification problems.\n",
        "# * Newton-CG: Suitable for larger datasets and multiclass classification problems.\n",
        "# * Sag: Suitable for large datasets and can be faster than other solvers.\n",
        "# * Saga: A variant of Sag that is more robust to noisy data.\n",
        "\n",
        "# The choice of solver depends on the size of the dataset, the type of classification problem (binary or multiclass), and computational resources.\n"
      ],
      "metadata": {
        "id": "67lTmEyu6Hk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "# Logistic Regression can be extended for multiclass classification using several approaches:\n",
        "\n",
        "# * One-vs-Rest (OvR): Trains a separate binary classifier for each class, where each classifier predicts the probability of an instance belonging to that class versus all other classes.\n",
        "# * Softmax Regression: A generalization of logistic regression for multiclass classification. It estimates the probability of an instance belonging to each class using a softmax function.\n"
      ],
      "metadata": {
        "id": "pufsvnhv6M9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "# Advantages:\n",
        "\n",
        "# * Relatively simple to understand and implement.\n",
        "# * Fast to train and predict.\n",
        "# * Can provide probabilistic outputs.\n",
        "# * Can be used for both binary and multiclass classification.\n",
        "\n",
        "# Disadvantages:\n",
        "\n",
        "# * Assumes a linear relationship between features and the log-odds of the outcome.\n",
        "# * Can be sensitive to outliers.\n",
        "# * May not perform well with complex datasets.\n",
        "# * Can be affected by class imbalance.\n"
      ],
      "metadata": {
        "id": "67RlvDOj6Q_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression?\n",
        "\n",
        "# Logistic Regression is used in various applications, including:\n",
        "\n",
        "# * Medical diagnosis (e.g., predicting the probability of a disease).\n",
        "# * Credit scoring (e.g., assessing the risk of loan default).\n",
        "# * Customer churn prediction (e.g., predicting whether a customer will leave a subscription service).\n",
        "# * Spam detection (e.g., classifying emails as spam or not spam).\n",
        "# * Image classification (e.g., identifying objects in images).\n"
      ],
      "metadata": {
        "id": "41OJ6xrR6YBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "# Logistic Regression is used for binary classification, predicting the probability of an instance belonging to one of two classes.\n",
        "# Softmax Regression is a generalization of logistic regression for multiclass classification, predicting the probability of an instance belonging to each of multiple classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_fRCB0E6dSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "# The choice between OvR and Softmax depends on the dataset and the desired outcome:\n",
        "\n",
        "# * OvR: Simpler to implement and can be more suitable for problems with a large number of classes.\n",
        "# * Softmax: Can provide more accurate predictions and better model interpretability by considering the relationships between all classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "nW5uRGl86kaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "# Coefficients in Logistic Regression represent the change in the log-odds of the outcome variable for a one-unit change in the corresponding predictor variable, while holding all other variables constant.\n",
        "# A positive coefficient indicates that an increase in the predictor variable increases the log-odds of the outcome variable, meaning the instance is more likely to belong to class 1.\n",
        "# A negative coefficient indicates that an increase in the predictor variable decreases the log-odds of the outcome variable, meaning the instance is less likely to belong to class 1.\n"
      ],
      "metadata": {
        "id": "_0VJL6476oDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical Questions**"
      ],
      "metadata": {
        "id": "OJr3fbmq6rne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "\n",
        "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "\n",
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using  LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "# 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "# 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "\n",
        "# 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "\n",
        "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "# 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "\n",
        "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n"
      ],
      "metadata": {
        "id": "bUpaLrmp6yiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1. Load dataset, split, apply Logistic Regression, print accuracy\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# 2. Apply L1 regularization (Lasso)\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model_l1.fit(X_train, y_train)\n",
        "\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
        "print(f\"Model accuracy with L1 regularization: {accuracy_l1}\")\n",
        "\n",
        "\n",
        "# 3. Apply L2 regularization (Ridge)\n",
        "model_l2 = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "print(f\"Model accuracy with L2 regularization: {accuracy_l2}\")\n",
        "print(f\"Model coefficients with L2 regularization: {model_l2.coef_}\")\n",
        "\n",
        "\n",
        "# 4. Apply Elastic Net Regularization\n",
        "# Note: Elastic Net requires a specific solver like 'saga'\n",
        "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model_elastic.fit(X_train, y_train)\n",
        "\n",
        "y_pred_elastic = model_elastic.predict(X_test)\n",
        "accuracy_elastic = accuracy_score(y_test, y_pred_elastic)\n",
        "print(f\"Model accuracy with Elastic Net regularization: {accuracy_elastic}\")\n",
        "\n",
        "\n",
        "\n",
        "# 5. Multiclass classification using 'ovr'\n",
        "model_ovr = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Model accuracy with multi_class='ovr': {accuracy_ovr}\")\n",
        "\n",
        "\n",
        "# 6. Apply GridSearchCV to tune hyperparameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "              'penalty': ['l1', 'l2']}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=1000), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_search.best_score_}\")\n",
        "\n",
        "\n",
        "# 7. Evaluate using Stratified K-Fold Cross-Validation\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=cv)\n",
        "\n",
        "print(f\"Average accuracy with Stratified K-Fold: {scores.mean()}\")\n",
        "\n",
        "\n",
        "# 8. Load dataset from CSV and apply Logistic Regression (example)\n",
        "# Replace 'your_dataset.csv' with the actual file path\n",
        "# Assuming the CSV has a column named 'target' for the target variable\n",
        "try:\n",
        "    data = pd.read_csv('your_dataset.csv')\n",
        "    X = data.drop('target', axis=1)\n",
        "    y = data['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy on the loaded dataset: {accuracy}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"CSV file not found. Please provide a valid path.\")\n",
        "\n",
        "\n",
        "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'C': uniform(loc=0.001, scale=10),  # Uniform distribution for C\n",
        "    'penalty': ['l1', 'l2'],  # Options for penalty\n",
        "    'solver': ['liblinear', 'saga']  # Options for solver\n",
        "}\n",
        "\n",
        "# Create a LogisticRegression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,  # Number of parameter settings that are sampled\n",
        "    cv=5,  # Number of cross-validation folds\n",
        "    random_state=42,  # Set a random seed for reproducibility\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by RandomizedSearchCV\n",
        "print(\"Best parameters found by RandomizedSearchCV: \", random_search.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the best model: \", accuracy)\n",
        "\n",
        "\n",
        "# 10. One-vs-One (OvO) Multiclass Logistic Regression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Create a base LogisticRegression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create an OvO classifier using the LogisticRegression model\n",
        "ovo_classifier = OneVsOneClassifier(model)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of One-vs-One Logistic Regression: \", accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# 11. Confusion Matrix Visualization for Binary Classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined for binary classification\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 12. Evaluate using Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-Score: \", f1)\n",
        "\n",
        "\n",
        "# 13. Logistic Regression with Class Weights for Imbalanced Data\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined and there is class imbalance\n",
        "\n",
        "# Calculate class weights (example: using 'balanced' strategy)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=model.classes_, y=y_train)\n",
        "class_weights_dict = dict(zip(model.classes_, class_weights))\n",
        "\n",
        "# Train the model with class weights\n",
        "model = LogisticRegression(max_iter=1000, class_weight=class_weights_dict)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model (you can use accuracy, precision, recall, F1-score, etc.)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with class weights: \", accuracy)\n",
        "\n",
        "# 14. Logistic Regression on Titanic Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Titanic dataset (replace with the actual file path)\n",
        "try:\n",
        "    titanic_data = pd.read_csv('titanic.csv')  # Assuming you have the titanic dataset\n",
        "\n",
        "    # Feature selection (example)\n",
        "    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "    X = titanic_data[features]\n",
        "    y = titanic_data['Survived']\n",
        "\n",
        "    # Handle missing values\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    X['Age'] = imputer.fit_transform(X[['Age']])\n",
        "\n",
        "    # Convert categorical features to numerical\n",
        "    X = pd.get_dummies(X, columns=['Sex', 'Embarked'], dummy_na=False)\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Train the Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy on Titanic dataset: \", accuracy)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Titanic dataset not found.\")\n",
        "\n",
        "# 15. Feature Scaling (Standardization)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model on scaled data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on scaled data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy on scaled data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy with feature scaling (Standardization): \", accuracy_scaled)\n",
        "\n",
        "# Train the model without scaling (for comparison)\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(\"Accuracy without feature scaling: \", accuracy_no_scaling)\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# 16. Train Logistic Regression and evaluate using ROC-AUC score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined for binary classification\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC score:\", roc_auc)\n",
        "\n",
        "\n",
        "# 17. Train Logistic Regression with custom learning rate and evaluate accuracy\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train the model with a custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)  # Lower C means stronger regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy with custom learning rate (C=0.5):\", accuracy)\n",
        "\n",
        "\n",
        "# 18. Identify important features based on model coefficients\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display feature importance\n",
        "feature_importance = pd.DataFrame({'Feature': iris.feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort by absolute coefficient value (larger magnitude indicates more importance)\n",
        "feature_importance = feature_importance.reindex(feature_importance.Coefficient.abs().sort_values(ascending=False).index)\n",
        "\n",
        "print(\"Feature Importance:\")\n",
        "print(feature_importance)\n",
        "\n",
        "\n",
        "# 19. Evaluate using Cohen's Kappa Score\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa)\n",
        "\n",
        "\n",
        "# 20. Visualize Precision-Recall Curve\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined for binary classification\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall for different thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Calculate area under the precision-recall curve\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, label=f'Precision-Recall AUC = {pr_auc:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 21. Compare accuracy with different solvers\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with solver {solver}: {accuracy}\")\n",
        "\n",
        "\n",
        "# 22. Evaluate using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "\n",
        "# 23. Train on raw and standardized data and compare accuracy\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Train on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Accuracy on raw data:\", accuracy_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"Accuracy on standardized data:\", accuracy_scaled)\n",
        "\n",
        "\n",
        "# 24. Find optimal C using cross-validation\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Define a range of C values\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
        "\n",
        "# Fit the model using GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best C value and its corresponding accuracy\n",
        "print(\"Best C value:\", grid_search.best_params_['C'])\n",
        "print(\"Best accuracy:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "# 25. Save and load the trained model using joblib\n",
        "\n",
        "from joblib import dump, load\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined and a model is trained\n",
        "\n",
        "# Save the trained model to a file\n",
        "dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load('logistic_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(\"Accuracy of the loaded model:\", accuracy_loaded)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LM_BcWHs7zXk",
        "outputId": "66054699-d2de-401b-960c-27114604af5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.0\n",
            "Model accuracy with L1 regularization: 1.0\n",
            "Model accuracy with L2 regularization: 1.0\n",
            "Model coefficients with L2 regularization: [[-0.39086505  0.92121451 -2.33169496 -0.9799741 ]\n",
            " [ 0.49862392 -0.30952772 -0.21642632 -0.7316387 ]\n",
            " [-0.10775887 -0.61168679  2.54812128  1.7116128 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy with Elastic Net regularization: 1.0\n",
            "Model accuracy with multi_class='ovr': 0.9736842105263158\n",
            "Best parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best accuracy: 0.9640316205533598\n",
            "Average accuracy with Stratified K-Fold: 0.9666666666666668\n",
            "CSV file not found. Please provide a valid path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by RandomizedSearchCV:  {'C': np.float64(8.325426408004217), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Accuracy of the best model:  1.0\n",
            "Accuracy of One-vs-One Logistic Regression:  1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d073fd1d512>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;31m# Calculate precision, recall, and F1-score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2245\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \"\"\"\n\u001b[0;32m-> 2247\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   2248\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     \"\"\"\n\u001b[1;32m   1829\u001b[0m     \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1830\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1614\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}