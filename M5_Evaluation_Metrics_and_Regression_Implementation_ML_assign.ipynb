{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What does R-squared represent in a regression model ?\n",
        "\n",
        "\n",
        "   R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
      ],
      "metadata": {
        "id": "emPQvQdAuF2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the assumptions of linear regression ?\n",
        "\n",
        "\n",
        "  linearity, independence of errors, homoscedasticity, normality of errors, absence of multicollinearity, and no autocorrelation—is critical for ensuring reliable results."
      ],
      "metadata": {
        "id": "SFwQVrsRuayB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between R-squared and Adjusted R-squared ?\n",
        "\n",
        "\n",
        "  R-squared measures the proportion of variance in the dependent variable explained by the independent variables, while adjusted R-squared adjusts for the number of predictors in the model, penalizing the addition of irrelevant variables and providing a more accurate measure of model fit, especially in multiple regression models."
      ],
      "metadata": {
        "id": "TU4D2cPrulE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Why do we use Mean Squared Error (MSE) ?\n",
        "\n",
        "\n",
        "In the fields of regression analysis and machine learning, the Mean Square Error (MSE) is a crucial metric for evaluating the performance of predictive models. It measures the average squared difference between the predicted and the actual target values within a dataset."
      ],
      "metadata": {
        "id": "vKVYtlLtuuTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What does an Adjusted R-squared value of 0.85 indicate ?\n",
        "\n",
        "\n",
        "  R-Squared of 0% indicates that the dependent variable cannot be predicted from the independent variable(s) at all. For instance, if the R-Squared of a model is 0.85, we can say that 85% of the variability in the output variable can be explained by the input variables that the model has used."
      ],
      "metadata": {
        "id": "CtiEAHUHu2f7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we check for normality of residuals in linear regression ?\n",
        "\n",
        "\n",
        "\n",
        "To check for normality of residuals in linear regression, you can use visual methods like histograms and Q-Q plots, or statistical tests like the Shapiro-Wilk test.\n",
        "\n",
        "1. Visual Methods:\n",
        "Histograms:\n",
        "A histogram shows the distribution of the residuals. If the residuals are normally distributed, the histogram should resemble a bell curve, with most values clustered around the mean.\n",
        "Q-Q Plot (Quantile-Quantile Plot):\n",
        "This plot compares the quantiles of the residuals to the quantiles of a normal distribution. If the residuals are normally distributed, the points in the Q-Q plot will fall along a straight line. Deviations from the line suggest non-normality.\n",
        "\n",
        "\n",
        "2. Statistical Tests:\n",
        "Shapiro-Wilk Test: This test formally assesses whether a sample comes from a normal distribution.\n",
        "Null Hypothesis: The residuals are normally distributed.\n",
        "Alternative Hypothesis: The residuals are not normally distributed.\n",
        "Interpretation: A low p-value (typically below 0.05) suggests that you should reject the null hypothesis and conclude that the residuals are not normally distributed.\n",
        "\n",
        "\n",
        "3. Steps for Checking Normality:\n",
        "Obtain Residuals: Calculate the difference between the observed and predicted values from your linear regression model.\n",
        "Visualize Residuals: Create a histogram and a Q-Q plot of the residuals.\n",
        "Perform Shapiro-Wilk Test: Use the Shapiro-Wilk test to formally test the normality of the residuals.\n",
        "Interpret Results: Based on the visual inspection and the p-value from the Shapiro-Wilk test, determine if the residuals are normally distributed.\n",
        "If Non-normality is Detected: Consider data transformations or alternative regression models."
      ],
      "metadata": {
        "id": "B0hhcXLivDm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is multicollinearity, and how does it impact regression ?\n",
        "\n",
        "\n",
        "Multicollinearity in regression analysis occurs when two or more independent variables are highly correlated, making it difficult to determine the individual effect of each variable on the dependent variable, and potentially leading to unstable and unreliable regression coefficient estimates.\n"
      ],
      "metadata": {
        "id": "M7F5Q-_wvZOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is Mean Absolute Error (MAE) ?\n",
        "\n",
        "\n",
        "\n",
        "  Mean Absolute Error (MAE) is a metric that measures the average magnitude of the errors between predicted and actual values, without considering the direction of the errors. It's calculated by averaging the absolute differences between predicted and actual values."
      ],
      "metadata": {
        "id": "fvq1ZmoOviLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the benefits of using an ML pipeline ?\n",
        "\n",
        "\n",
        "   ML pipelines benefit performance and organization. This type of ML pipeline improves the performance and organization of the entire model portfolio, getting models into production quicker and making managing machine learning models easier."
      ],
      "metadata": {
        "id": "S3L0J0Wavyex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Why is RMSE considered more interpretable than MSE ?\n",
        "\n",
        "\n",
        "\n",
        "   MSE (Root Mean Squared Error) is often preferred over MSE (Mean Squared Error) because RMSE is expressed in the same units as the original data, making it more interpretable and easier to understand the magnitude of errors, while MSE is in squared units."
      ],
      "metadata": {
        "id": "70S7d4GDv9i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is pickling in Python, and how is it useful in ML ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   \"pickling\" refers to the process of serializing Python objects (like data structures or ML models) into a byte stream for storage or transmission, and then \"unpickling\" (deserializing) them back into their original form. This is useful in Machine Learning to save trained models for later use, avoiding retraining."
      ],
      "metadata": {
        "id": "4AfOgMbUwJG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What does a high R-squared value mean ?\n",
        "\n",
        "\n",
        "   A high R-squared value (close to 1) in a regression model indicates that the model effectively explains a large proportion of the variability in the dependent variable based on the independent variables."
      ],
      "metadata": {
        "id": "-MGVBV21wSZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What happens if linear regression assumptions are violated ?\n",
        "\n",
        "\n",
        "  Violating the assumptions of linear regression can lead to biased or inefficient estimates, unreliable confidence intervals, and potentially misleading conclusions about the relationship between variables."
      ],
      "metadata": {
        "id": "ZZZDea9Vwas3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can we address multicollinearity in regression ?\n",
        "\n",
        "\n",
        "To address multicollinearity in regression, you can remove highly correlated variables, combine them into a single predictor, use dimensionality reduction techniques like Principal Component Analysis (PCA), or employ regularization techniques like Ridge or Lasso regression.\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "Et0TGZFxwipO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can feature selection improve model performance in regression analysis ?\n",
        "\n",
        "\n",
        "\n",
        "  Feature selection in regression analysis improves model performance by identifying and retaining only the most relevant features, reducing complexity, preventing overfitting, and enhancing interpretability and computational efficiency."
      ],
      "metadata": {
        "id": "AsKsFkwOxfUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  How is Adjusted R-squared calculated ?\n",
        "\n",
        "\n",
        "Adjusted R-squared is calculated by taking the R-squared value and adjusting it to account for the number of predictors (independent variables) in the model, using the formula: 1 - [(1 - R²) * (n - 1) / (n - k - 1)], where 'n' is the number of data points, 'k' is the number of predictors, and R² is the R-squared value."
      ],
      "metadata": {
        "id": "MRs904VxxsQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is MSE sensitive to outliers ?\n",
        "\n",
        "\n",
        "MSE (Mean Squared Error) is sensitive to outliers because it squares the errors, meaning larger errors are amplified and contribute disproportionately to the overall MSE value, making the metric less robust to extreme values."
      ],
      "metadata": {
        "id": "REi5kS6zx3ZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of homoscedasticity in linear regression ?\n",
        "\n",
        "\n",
        "  In linear regression, homoscedasticity, meaning constant variance of the error terms, is a crucial assumption that ensures the reliability and validity of the model's results, leading to unbiased and efficient estimates of the regression coefficients."
      ],
      "metadata": {
        "id": "bT3SCITpyCKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  What is Root Mean Squared Error\n",
        "     (RMSE) ?\n",
        "\n",
        "\n",
        "      RMSE represents the square root of the average squared differences between predicted and observed outcomes. It is a metric predominantly utilized in regression analysis and forecasting, where accuracy matters significantly. The lower the RMSE, the better the model’s ability to predict accurately. Conversely, a higher RMSE signifies a greater discrepancy between the predicted and actual outcomes.\n",
        "      \n",
        "      \n",
        "      RMSE = sqrt [(Σ(Pi – Oi)²) / n]\n"
      ],
      "metadata": {
        "id": "Y5MEYI1SyMos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Why is pickling considered risky ?\n",
        "\n",
        "\n",
        "\n",
        "  Pickling can be considered risky due to the potential for high sodium content, the possibility of bacterial growth (including botulism), and the formation of carcinogenic compounds during fermentation."
      ],
      "metadata": {
        "id": "XjH0QfVoyouB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What alternatives exist to pickling for saving ML models ?\n",
        "\n",
        "\n",
        "\n",
        "Other serialization formats such as JSON, YAML, XML, and Protocol Buffers offer alternatives to pickle and joblib for saving models."
      ],
      "metadata": {
        "id": "Zvhu6gy2yy1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  What is heteroscedasticity, and why is it a problem ?\n",
        "\n",
        "\n",
        "\n",
        "Heteroscedasticity, in the context of regression analysis, refers to a situation where the variance of the errors (residuals) is not constant across all levels of the independent variable(s), meaning the spread of the data points around the regression line is not consistent. This violates a key assumption of standard regression models and leads to unreliable hypothesis tests and inaccurate confidence intervals."
      ],
      "metadata": {
        "id": "zB5yxCNJzBRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. How can interaction terms enhance a regression model's predictive power?\n",
        "\n",
        "\n",
        "\n",
        "Interaction terms enhance a regression model's predictive power by allowing the model to capture more complex relationships between variables, where the effect of one predictor on the outcome depends on the value of another predictor, leading to a better fit and potentially more accurate predictions."
      ],
      "metadata": {
        "id": "f11yzu74zLTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "     **Practical:**"
      ],
      "metadata": {
        "id": "tDFtAgi1zWPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model\n",
        "using Seaborn's \"diamonds\" dataset."
      ],
      "metadata": {
        "id": "zpDa_HqWzssC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Define features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table', 'x', 'y', 'z']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate residuals\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Visualize the distribution of residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8wUvQlTvzw-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root\n",
        "Mean Squared Error (RMSE) for a linear regression model."
      ],
      "metadata": {
        "id": "R22bmGMi2uHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")"
      ],
      "metadata": {
        "id": "AggSvcb72xts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check\n",
        "linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."
      ],
      "metadata": {
        "id": "0shiLHZQ363G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check linearity\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "plt.show()\n",
        "\n",
        "# Check homoscedasticity\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs. Predicted Values\")\n",
        "plt.show()\n",
        "\n",
        "# Check multicollinearity\n",
        "correlation_matrix = X.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aP8XG15A3-jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the\n",
        "performance of different regression models"
      ],
      "metadata": {
        "id": "tqa0tR5e4Isu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a pipeline with feature scaling and a linear regression model\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Define different regression models\n",
        "models = [\n",
        "    ('Linear Regression', LinearRegression()),\n",
        "    ('Ridge Regression', Ridge()),\n",
        "    ('Lasso Regression', Lasso())\n",
        "]\n",
        "\n",
        "# Evaluate the performance of each model using cross-validation\n",
        "for name, model in models:\n",
        "    pipeline.set_params(model=model)\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
        "    print(f\"{name}: Mean R-squared = {scores.mean():.3f}, Standard Deviation = {scores.std():.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "78TG2DzJ4L7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and\n",
        "R-squared score."
      ],
      "metadata": {
        "id": "gYBEtii64b6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Fit a simple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train[['carat']], y_train)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "y_pred = model.predict(X_test[['carat']])\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "GWkh1kvR4gAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using\n",
        "simple linear regression and visualizes the results."
      ],
      "metadata": {
        "id": "kQyw7nyw4nXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tips = sns.load_dataset('tips')\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
        "plt.plot(X_test, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('Total Bill')\n",
        "plt.ylabel('Tip')\n",
        "plt.title('Relationship between Total Bill and Tip')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "LpSMjje54qUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the\n",
        "model to predict new values and plot the data points along with the regression line."
      ],
      "metadata": {
        "id": "q9K9TYrx4yCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(100, 1)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G0YrdTkI41gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python script that pickles a trained linear regression model and saves it to a file."
      ],
      "metadata": {
        "id": "3WU_Xn_E459q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Train a model (example)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to a file\n",
        "filename = 'linear_regression_model.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "qqKgyn784-3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the\n",
        "regression curve."
      ],
      "metadata": {
        "id": "XJq16bJd5EMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Generate synthetic data (example)\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.randn(40) * 0.1\n",
        "\n",
        "# Create polynomial features\n",
        "polynomial_features = PolynomialFeatures(degree=2)\n",
        "X_poly = polynomial_features.fit_transform(X)\n",
        "\n",
        "# Fit a linear regression model to the polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "X_new_poly = polynomial_features.transform(X_new)\n",
        "y_pred = model.predict(X_new_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Polynomial Regression (Degree 2)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Gr-AETiU5HD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10. Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear\n",
        "regression model to the data. Print the model's coefficient and intercept."
      ],
      "metadata": {
        "id": "P3Zfn_535O6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(100, 1)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "print(\"Coefficient:\", model.coef_[0][0])\n",
        "print(\"Intercept:\", model.intercept_[0])"
      ],
      "metadata": {
        "id": "9PnBENhG5Rwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 11. Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and\n",
        "compares their performance."
      ],
      "metadata": {
        "id": "iAyuobsi5VqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "# 11. Fit polynomial regression models of different degrees to a synthetic dataset and compare their performance.\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.randn(80) * 0.1\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit polynomial regression models of different degrees\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "for degree in degrees:\n",
        "    polynomial_features = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = polynomial_features.fit_transform(X_train)\n",
        "    X_test_poly = polynomial_features.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Degree {degree}: R-squared = {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "Q4sQq_sC5a73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 12. Write a Python script that fits a simple linear regression model with two features and prints the model's\n",
        "coefficients, intercept, and R-squared score."
      ],
      "metadata": {
        "id": "yEaLBIGe5gNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with two features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2) * 10\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] + 1 + np.random.randn(100) * 2\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a simple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's coefficients, intercept, and R-squared score\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "print(\"Intercept:\", model.intercept_)\n",
        "y_pred = model.predict(X_test)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "id": "8L6mFI7D5imo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 13. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the\n",
        "regression line along with the data points.\n",
        ""
      ],
      "metadata": {
        "id": "uxr1c3Ql5smw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with one feature\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(100, 1)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9AKWbi_c5vGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset\n",
        "with multiple features"
      ],
      "metadata": {
        "id": "tfJgFdr05-Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Load or generate a dataset with multiple features\n",
        "# ... (Replace with your dataset or generate synthetic data)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
        "\n",
        "# Print the VIF values\n",
        "print(vif_data)\n",
        "\n",
        "# Interpret VIF values:\n",
        "# VIF > 10 indicates high multicollinearity."
      ],
      "metadata": {
        "id": "Pd1zHonh6FMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a\n",
        "polynomial regression model, and plots the regression curve."
      ],
      "metadata": {
        "id": "RNaIlpFI6bN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with a polynomial relationship\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = X**4 + 2 * X**3 - 3 * X**2 + X + np.random.randn(80) * 5\n",
        "\n",
        "# Create polynomial features\n",
        "polynomial_features = PolynomialFeatures(degree=4)\n",
        "X_poly = polynomial_features.fit_transform(X)\n",
        "\n",
        "# Fit a linear regression model to the polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "X_new_poly = polynomial_features.transform(X_new)\n",
        "y_pred = model.predict(X_new_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Polynomial Regression (Degree 4)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FJ3R9ye76enM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple\n",
        "linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "QUgC-nVG6s9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data for multiple linear regression\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - 1 * X[:, 2] + 1 + np.random.randn(100) * 2\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with data standardization and multiple linear regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize the data\n",
        "    ('regressor', LinearRegression())  # Multiple linear regression model\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "y3yZC9DK6wvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 17. Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the\n",
        "regression curve."
      ],
      "metadata": {
        "id": "vXIAVey066pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with a polynomial relationship\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = X**3 + 2 * X**2 - 3 * X + np.random.randn(80) * 5\n",
        "\n",
        "# Create polynomial features\n",
        "polynomial_features = PolynomialFeatures(degree=3)\n",
        "X_poly = polynomial_features.fit_transform(X)\n",
        "\n",
        "# Fit a linear regression model to the polynomial features\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "X_new_poly = polynomial_features.transform(X_new)\n",
        "y_pred = model.predict(X_new_poly)\n",
        "\n",
        "# Plot the data points and the regression curve\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Curve')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Polynomial Regression (Degree 3)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VeiT-_pS69ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print\n",
        "the R-squared score and model coefficients."
      ],
      "metadata": {
        "id": "j_h8UJCH7ECE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with 5 features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5) * 10\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - 1 * X[:, 2] + 0.5 * X[:, 3] - 2 * X[:, 4] + 1 + np.random.randn(100) * 2\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(f\"Model coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "r1BCMejw7Hex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 19. Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the\n",
        "data points along with the regression line."
      ],
      "metadata": {
        "id": "2asCBhXD7OFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with one feature\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(100, 1)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict new values\n",
        "X_new = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "# Plot the data points and the regression line\n",
        "plt.scatter(X, y, color='blue', label='Data Points')\n",
        "plt.plot(X_new, y_pred, color='red', label='Regression Line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Linear Regression on Synthetic Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oAPjuBif7Qwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R\n",
        "squared score and coefficients."
      ],
      "metadata": {
        "id": "Yb2KOHSH7Xzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data with 3 features\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 3) * 10\n",
        "y = 2 * X[:, 0] + 3 * X[:, 1] - 1 * X[:, 2] + 1 + np.random.randn(100) * 2\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")\n",
        "\n",
        "# Print the model coefficients\n",
        "print(f\"Model coefficients: {model.coef_}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DAHDE-xJ7bOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python script that demonstrates how to serialize and deserialize machine learning models using\n",
        "joblib instead of pickling."
      ],
      "metadata": {
        "id": "obdwvuiH7iWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model (example)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Serialize the model using joblib\n",
        "import joblib\n",
        "filename = 'linear_regression_model.joblib'\n",
        "joblib.dump(model, filename)\n",
        "\n",
        "# Deserialize the model using joblib\n",
        "loaded_model = joblib.load(filename)\n",
        "\n",
        "# Use the loaded model for predictions\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "# Check if the predictions are the same\n",
        "assert np.allclose(y_pred, y_pred_loaded)\n"
      ],
      "metadata": {
        "id": "JjahKlOd7mBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use\n",
        "the Seaborn 'tips' dataset"
      ],
      "metadata": {
        "id": "j0hMlhav7x5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tips dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Create dummy variables for categorical features (e.g., 'sex', 'smoker', 'day', 'time')\n",
        "tips = pd.get_dummies(tips, columns=['sex', 'smoker', 'day', 'time'], drop_first=True)\n",
        "\n",
        "# Define features and target variable\n",
        "X = tips[['total_bill', 'size', 'sex_Male', 'smoker_Yes', 'day_Sat', 'day_Sun', 'day_Thur', 'time_Dinner']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared score: {r2}\")\n"
      ],
      "metadata": {
        "id": "Uf516ncL70or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R\n",
        "squared score."
      ],
      "metadata": {
        "id": "7afos8T38U0E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7LdBj-T8XrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic\n",
        "dataset."
      ],
      "metadata": {
        "id": "qDh-rQU08exh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2 * X.ravel() + 1 + np.random.randn(100) * 2\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Perform cross-validation (e.g., 5-fold)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation R-squared scores:\", cv_scores)\n",
        "print(\"Mean R-squared:\", cv_scores.mean())"
      ],
      "metadata": {
        "id": "kZDLtF5e8h8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python script that compares polynomial regression models of different degrees and prints the R\n",
        "squared score for each."
      ],
      "metadata": {
        "id": "H2HPBYK58mSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.randn(80) * 0.1\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit polynomial regression models of different degrees\n",
        "degrees = [1, 2, 3, 4, 5]\n",
        "for degree in degrees:\n",
        "    polynomial_features = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = polynomial_features.fit_transform(X_train)\n",
        "    X_test_poly = polynomial_features.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Degree {degree}: R-squared = {r2:.4f}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EtpVgW4V8pUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}