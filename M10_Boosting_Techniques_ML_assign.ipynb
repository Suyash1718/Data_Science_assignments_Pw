{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is Boosting in Machine Learning ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Boosting is a machine learning ensemble method that sequentially combines \"weak learners\" (models that are only slightly better than random guessing) to create a strong, accurate predictive model by focusing on correcting errors made by previous models."
      ],
      "metadata": {
        "id": "jpZbdbvA6HxH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69nOwnWiXQXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How does Boosting differ from Bagging ?\n",
        "\n",
        "\n",
        "\n",
        "Bagging (Bootstrap Aggregating) trains models in parallel on different subsets of the data, averaging their predictions to reduce variance, while Boosting trains models sequentially, with each model focusing on the errors of the previous ones to reduce bias and improve accuracy."
      ],
      "metadata": {
        "id": "dVy9fskx6anE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the key idea behind AdaBoost ?\n",
        "\n",
        "\n",
        "The core idea behind AdaBoost (Adaptive Boosting) is to sequentially train multiple \"weak learners\" (simple models) and combine their predictions to create a strong learner. It works by assigning weights to training examples, focusing on misclassified instances in subsequent iterations, effectively \"boosting\" the performance of the weak learners."
      ],
      "metadata": {
        "id": "MgF1mZgJ6vBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the working of AdaBoost with an example.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "AdaBoost is an ensemble learning algorithm that sequentially trains weak learners, focusing on correcting mistakes from previous learners. It works by assigning weights to training examples, increasing the weight of misclassified instances, and then training new learners that pay more attention to these misclassified examples. This process is repeated, and the final prediction is a weighted combination of the predictions from all the weak learners."
      ],
      "metadata": {
        "id": "1Vbb1cuX69Qu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Gradient Boosting, and how is it different from AdaBoost ?\n",
        "\n",
        "\n",
        "Gradient boosting and AdaBoost are both ensemble methods that sequentially build models, focusing on correcting errors from previous models, but they differ in how they identify and address these errors. AdaBoost uses a specific loss function and reweights misclassified samples, while Gradient Boosting is more flexible, using gradients to find approximate solutions and can handle more complex base learners."
      ],
      "metadata": {
        "id": "9z-9ZVio7Ixe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  What is the loss function in Gradient Boosting ?\n",
        "\n",
        "\n",
        "In gradient boosting, the loss function quantifies the difference between model predictions and actual values, guiding the algorithm to minimize errors iteratively. For regression tasks, common loss functions include mean squared error (MSE), while for classification, logarithmic loss (or cross-entropy) is often used."
      ],
      "metadata": {
        "id": "6NDf07mI7UYn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  How does XGBoost improve over traditional Gradient Boosting ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "XGBoost improves upon traditional Gradient Boosting by incorporating L1 and L2 regularization, parallel processing for speed, and sparse-aware split finding to handle missing values efficiently. This results in improved model performance, reduced overfitting, and faster training, making it a more versatile and powerful algorithm.\n"
      ],
      "metadata": {
        "id": "2Cs_ifpi7d02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the difference between XGBoost and CatBoos ?\n",
        "\n",
        "\n",
        "XGBoost and CatBoost are both powerful gradient boosting algorithms, but they differ primarily in their handling of categorical data and their overall approach to model building. XGBoost requires preprocessing of categorical features, while CatBoost natively handles them, potentially leading to simpler workflows and improved performance, especially with datasets rich in categorical variables.\n"
      ],
      "metadata": {
        "id": "02_q1RLL7tF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are some real-world applications of Boosting techniques ?\n",
        "\n",
        "\n",
        "Boosting techniques find applications in diverse fields, enhancing accuracy and performance in various tasks. Some key areas include finance, healthcare, e-commerce, and natural language processing. In finance, they help with credit scoring, fraud detection, and stock market prediction. Healthcare uses them for disease diagnosis, patient risk assessment, and medication development. E-commerce leverages boosting for personalized recommendations and customer segmentation. Natural language processing benefits from boosting in tasks like text classification and sentiment analysis."
      ],
      "metadata": {
        "id": "Dh4KyMkG75oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.   How does regularization help in XGBoost ?\n",
        "\n",
        "\n",
        "Regularization in XGBoost helps prevent overfitting by adding penalties to the model's complexity, encouraging it to favor simpler trees and improve generalization to unseen data. This is achieved through L1 (Lasso) and L2 (Ridge) regularization terms added to the objective function."
      ],
      "metadata": {
        "id": "V_kfxCFJ8EEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What are some hyperparameters to tune in Gradient Boosting models?\n",
        "\n",
        "\n",
        "\n",
        "The most important XGBoost hyperparameters to tune are:\n",
        "max_depth : Maximum depth of a tree. ...\n",
        "min_child_weight : Minimum sum of instance weight needed in a child. ...\n",
        "subsample : Subsample ratio of the training instances. ...\n",
        "colsample_bytree : Subsample ratio of columns when constructing each tree."
      ],
      "metadata": {
        "id": "RRIlYJhk8VyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  What is the concept of Feature Importance in Boosting ?\n",
        "\n",
        "\n",
        "\n",
        "In the context of boosting algorithms, feature importance refers to a method of quantifying how much each input feature contributes to the model's predictive power. It essentially provides a score or ranking for each feature, indicating its relative usefulness in making predictions. This helps in understanding which features are most influential and can guide feature selection for model simplification and improvement."
      ],
      "metadata": {
        "id": "L4ceQIE_8uHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Why is CatBoost efficient for categorical data?\n",
        "\n",
        "\n",
        "\n",
        "CatBoost is efficient for categorical data because it handles categorical features directly without requiring extensive preprocessing, such as one-hot encoding. It uses a novel algorithm called \"target statistics\" to convert categorical values into numerical representations, considering the target variable to reduce overfitting and improve accuracy. Additionally, CatBoost's \"Ordered Boosting\" feature helps in capturing complex relationships within categorical data, leading to robust predictive models."
      ],
      "metadata": {
        "id": "Jh-V2fYb87lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#14.  Train an AdaBoost Classifier on a sample dataset and print model accuracy\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "        iris = pd.read_csv('/kaggle/input/iris/Iris.csv')\n",
        "\n",
        "\n",
        "\n",
        "        X = iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
        "\n",
        "X.head()\n",
        "\n",
        "\n",
        "\n",
        "y = iris['Species']\n",
        "\n",
        "y.head()"
      ],
      "metadata": {
        "id": "s3vUd98z7qnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15.  Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "\n",
        "\n",
        "# check scikit-learn version\n",
        "import sklearn\n",
        "\n",
        "\n",
        "# test classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "\n",
        "# evaluate adaboost algorithm for classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
        "# define the model\n",
        "model = AdaBoostClassifier()\n",
        "# evaluate the model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3xm_hk2-AS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16.  Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "DISPLAY_PRECISION = 4\n",
        "\n",
        "pd.set_option(\"display.precision\", DISPLAY_PRE\n",
        "\n",
        "\n",
        "              dat = datasets.load_breast_cancer()\n",
        "\n",
        "\n",
        "              print(\"The sklearn breast cancer dataset keys:\")\n",
        "print(dat.keys()) # dict_keys(['target_names', 'target', 'feature_names', 'data', 'DESCR'])\n",
        "print(\"---\")\n",
        "\n",
        "# Note that we need to reverse the original '0' and '1' mapping in order to end up with this mapping:\n",
        "# Benign = 0 (negative class)\n",
        "# Malignant = 1 (positive class)\n",
        "\n",
        "li_classes = [dat.target_names[1], dat.target_names[0]]\n",
        "li_target = [1 if x==0 else 0 for x in list(dat.target)]\n",
        "li_ftrs = list(dat.feature_names)\n",
        "\n",
        "print(\"There are 2 target classes:\")\n",
        "print(\"li_classes\", li_classes)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"Target class distribution from a total of %d target values:\" % len(li_target))\n",
        "print(pd.Series(li_target).value_counts())\n",
        "print(\"---\")\n",
        "\n",
        "df_all = pd.DataFrame(dat.data[:,:], columns=li_ftrs)\n",
        "print(\"Describe dataframe, first 6 columns:\")\n",
        "print(df_all.iloc[:,:6].describe().to_string())"
      ],
      "metadata": {
        "id": "xLTELefv_AMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample dataset: House features (Square Footage, Number of Bedrooms) and Prices\n",
        "X = np.array([[1500, 3], [1800, 4], [2400, 3], [3000, 5], [3500, 4]])  # Features\n",
        "y = np.array([400000, 450000, 600000, 650000, 700000])  # House prices\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"R-squared (R2 Score): {r2:.2f}\")\n",
        "\n",
        "# Visualize Actual vs. Predicted Prices\n",
        "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual Prices')\n",
        "plt.scatter(range(len(y_pred)), y_pred, color='red', label='Predicted Prices')\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Actual vs Predicted Prices')\n",
        "plt.xlabel('Test Sample Index')\n",
        "plt.ylabel('House Price')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "63LPe9AQXSUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = 'C:/datasets/Wholesale customers data.csv'\n",
        "\n",
        "df = pd.read_csv(data)\n",
        "df.shape\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gtGDn3CaYeGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Train a CatBoost Classifier and evaluate using F1-Score\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# initialize data\n",
        "train_data = np.random.randint(0,\n",
        "                               100,\n",
        "                               size=(100, 10))\n",
        "\n",
        "train_labels = np.random.randint(0,\n",
        "                                 2,\n",
        "                                 size=(100))\n",
        "\n",
        "test_data = catboost_pool = Pool(train_data,\n",
        "                                 train_labels)\n",
        "\n",
        "model = CatBoostClassifier(iterations=2,\n",
        "                           depth=2,\n",
        "                           learning_rate=1,\n",
        "                           loss_function='Logloss',\n",
        "                           verbose=True)\n",
        "# train the model\n",
        "model.fit(train_data, train_labels)\n",
        "# make the prediction using the resulting model\n",
        "preds_class = model.predict(test_data)\n",
        "preds_proba = model.predict_proba(test_data)\n",
        "print(\"class = \", preds_class)\n",
        "print(\"proba = \", preds_proba)\n",
        "\n",
        "\n",
        "CatBoostRegressor\n",
        "import numpy as np\n",
        "from catboost import Pool, CatBoostRegressor\n",
        "# initialize data\n",
        "train_data = np.random.randint(0,\n",
        "                               100,\n",
        "                               size=(100, 10))\n",
        "train_label = np.random.randint(0,\n",
        "                                1000,\n",
        "                                size=(100))\n",
        "test_data = np.random.randint(0,\n",
        "                              100,\n",
        "                              size=(50, 10))\n",
        "# initialize Pool\n",
        "train_pool = Pool(train_data,\n",
        "                  train_label,\n",
        "                  cat_features=[0,2,5])\n",
        "test_pool = Pool(test_data,\n",
        "                 cat_features=[0,2,5])\n",
        "\n",
        "# specify the training parameters\n",
        "model = CatBoostRegressor(iterations=2,\n",
        "                          depth=2,\n",
        "                          learning_rate=1,\n",
        "                          loss_function='RMSE')\n",
        "#train the model\n",
        "model.fit(train_pool)\n",
        "# make the prediction using the resulting model\n",
        "preds = model.predict(test_pool)\n",
        "print(preds)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from catboost import CatBoost, Pool\n",
        "\n",
        "# read the dataset\n",
        "\n",
        "train_data = np.random.randint(0,\n",
        "                               100,\n",
        "                               size=(100, 10))\n",
        "train_labels = np.random.randint(0,\n",
        "                                2,\n",
        "                                size=(100))\n",
        "test_data = np.random.randint(0,\n",
        "                                100,\n",
        "                                size=(50, 10))\n",
        "\n",
        "train_pool = Pool(train_data,\n",
        "                  train_labels)\n",
        "\n",
        "test_pool = Pool(test_data)\n",
        "# specify training parameters via map\n",
        "\n",
        "param = {'iterations':5}\n",
        "model = CatBoost(param)\n",
        "#train the model\n",
        "model.fit(train_pool)\n",
        "# make the prediction using the resulting model\n",
        "preds_class = model.predict(test_pool, prediction_type='Class')\n",
        "preds_proba = model.predict(test_pool, prediction_type='Probability')\n",
        "preds_raw_vals = model.predict(test_pool, prediction_type='RawFormulaVal')\n",
        "print(\"Class\", preds_class)\n",
        "print(\"Proba\", preds_proba)\n",
        "print(\"Raw\", preds_raw_vals)"
      ],
      "metadata": {
        "id": "ggmubtjPZAPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20  Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "\n",
        "# Prepare features and target\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load stock data\n",
        "stock_data = pd.read_csv('stock_data.csv')\n",
        "\n",
        "# Calculate technical indicators (e.g., Moving Average)\n",
        "stock_data['MA_7'] = stock_data['Close'].rolling(window=7).mean()\n",
        "stock_data['MA_21'] = stock_data['Close'].rolling(window=21).mean()\n",
        "\n",
        "# Prepare features and target\n",
        "X = stock_data[['Open', 'High', 'Low', 'Volume', 'MA_7', 'MA_21']].dropna()\n",
        "y = stock_data['Close'].dropna()\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "split = int(0.8 * len(X_scaled))\n",
        "X_train, X_test = X_scaled[:split], X_scaled[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"Root Mean Squared Error: ${rmse:.2f}\")"
      ],
      "metadata": {
        "id": "h-UcTq4FZe_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 #Import libraries:\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn import metrics   #Additional scklearn functions\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4\n",
        "\n",
        "train = pd.read_csv('Train_Modified.csv', encoding='ISO-8859–1')\n",
        "target = 'Disbursed'\n",
        "IDcol = 'ID'\n",
        "\n",
        "print(\"There will be no output for this particular block of code\")\n",
        "\n",
        "\n",
        "\n",
        "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "\n",
        "    if useTrainCV:\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
        "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "\n",
        "    #Fit the algorithm on the data\n",
        "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
        "\n",
        "    #Predict training set:\n",
        "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
        "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
        "\n",
        "    #Print model report:\n",
        "    print \"\\nModel Report\"\n",
        "    print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions)\n",
        "    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob)\n",
        "\n",
        "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
        "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "    plt.ylabel('Feature Importance Score')"
      ],
      "metadata": {
        "id": "WsmhiLYpZ5Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28  Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
        "#Ploting barplot for target\n",
        "plt.figure(figsize=(10,6))\n",
        "g = sns.barplot(data['stroke'], data['stroke'], palette='Set1', estimator=lambda x: len(x) / len(data) )\n",
        "\n",
        "#Anotating the graph\n",
        "for p in g.patches:\n",
        "        width, height = p.get_width(), p.get_height()\n",
        "        x, y = p.get_xy()\n",
        "        g.text(x+width/2,\n",
        "               y+height,\n",
        "               '{:.0%}'.format(height),\n",
        "               horizontalalignment='center',fontsize=15)\n",
        "\n",
        "#Setting the labels\n",
        "plt.xlabel('Heart Stroke', fontsize=14)\n",
        "plt.ylabel('Precentage', fontsize=14)\n",
        "plt.title('Percentage of patients will/will not have heart stroke', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#Training the model using mode of target\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "pred_test = []\n",
        "for i in range (0, 13020):\n",
        "    pred_test.append(y_train.mode()[0])\n",
        "\n",
        "#Printing f1 and accuracy scores\n",
        "print('The accuracy for mode model is:', accuracy_score(y_test, pred_test))\n",
        "print('The f1 score for the model model is:',f1_score(y_test, pred_test))\n",
        "\n",
        "#Ploting the cunfusion matrix\n",
        "conf_matrix(y_test, pred_test)"
      ],
      "metadata": {
        "id": "4BZIludfaTHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Train an AdaBoost Classifier and analyze the effect of different learning rates\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        self.polarity = 1\n",
        "        self.feature_idx = None\n",
        "        self.threshold = None\n",
        "        self.alpha = None\n",
        "\n",
        "    def predict(self, X):\n",
        "        n_samples = X.shape[0]\n",
        "        predictions = np.ones(n_samples)\n",
        "        feature_column = X[:, self.feature_idx]\n",
        "\n",
        "        if self.polarity == 1:\n",
        "            predictions[feature_column < self.threshold] = -1\n",
        "        else:\n",
        "            predictions[feature_column > self.threshold] = -1\n",
        "\n",
        "        return predictions\n",
        "\n",
        "class AdaBoost:\n",
        "    def __init__(self, n_clf=5):\n",
        "        self.n_clf = n_clf\n",
        "        self.clfs = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "        for _ in range(self.n_clf):\n",
        "            clf = DecisionStump()\n",
        "            min_error = float('inf')\n",
        "\n",
        "            for feature_i in range(n_features):\n",
        "                X_column = X[:, feature_i]\n",
        "                thresholds = np.unique(X_column)\n",
        "\n",
        "                for threshold in thresholds:\n",
        "                    predictions = np.ones(n_samples)\n",
        "                    predictions[X_column < threshold] = -1\n",
        "\n",
        "                    error = sum(w[y != predictions])\n",
        "\n",
        "                    if error > 0.5:\n",
        "                        error = 1 - error\n",
        "                        p = -1\n",
        "                    else:\n",
        "                        p = 1\n",
        "\n",
        "                    if error < min_error:\n",
        "                        clf.polarity = p\n",
        "                        clf.threshold = threshold\n",
        "                        clf.feature_idx = feature_i\n",
        "                        min_error = error\n",
        "\n",
        "            EPS = 1e-10\n",
        "            clf.alpha = 0.5 * np.log((1.0 - min_error + EPS) / (min_error + EPS))\n",
        "            predictions = clf.predict(X)\n",
        "            w *= np.exp(-clf.alpha * y * predictions)\n",
        "            w /= np.sum(w)\n",
        "            self.clfs.append(clf)\n",
        "\n",
        "    def predict(self, X):\n",
        "        clf_preds = [clf.alpha * clf.predict(X) for clf in self.clfs]\n",
        "        y_pred = np.sum(clf_preds, axis=0)\n",
        "        return np.sign(y_pred)\n",
        "\n",
        "\n",
        "        from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"Iris.csv\")  # Adjust the file path as necessary\n",
        "X = data.iloc[:, :-1].values  # Features\n",
        "y = data.iloc[:, -1].values  # Target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the AdaBoost classifier\n",
        "abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50)\n",
        "\n",
        "# Fit the model\n",
        "abc.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = abc.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "nBw4ynXJavkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import xgboost as xgb\n",
        "\n",
        "dbunch = datasets.load_breast_cancer(as_frame=True)\n",
        "df = dbunch.frame\n",
        "features = dbunch.feature_names\n",
        "target_names = dbunch.target_names\n",
        "target = 'target'\n",
        "df.info()\n",
        "\n",
        "\n",
        "\n",
        "df.target.value_counts().sort_index().plot.bar()\n",
        "plt.xlabel('target')\n",
        "plt.ylabel('count');\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_valid = 50\n",
        "\n",
        "train_df, valid_df = train_test_split(df, test_size=n_valid, random_state=42)\n",
        "train_df.shape, valid_df.shape\n",
        "\n",
        "params = {\n",
        "    'tree_method': 'exact',\n",
        "    'objective': 'binary:logistic',\n",
        "}\n",
        "num_boost_round = 50\n",
        "\n",
        "dtrain = xgb.DMatrix(label=train_df[target], data=train_df[features])\n",
        "dvalid = xgb.DMatrix(label=valid_df[target], data=valid_df[features])\n",
        "model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_boost_round,\n",
        "                  evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
        "                  verbose_eval=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_true = valid_df[target]\n",
        "y_pred = clf.predict(valid_df[features])\n",
        "y_score = clf.predict_proba(valid_df[features])[:,1]\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "metrics.accuracy_score(y_true, y_pred)\n",
        "print(metrics.classification_report(y_true, y_pred, target_names=target_names))\n",
        "metrics.roc_auc_score(y_true, y_score)\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "scorer = make_scorer(metrics.log_loss, greater_is_better=False, needs_proba=True)\n",
        "permu_imp = permutation_importance(clf, valid_df[features], valid_df[target],\n",
        "                                   n_repeats=30, random_state=0, scoring=scorer)\n",
        "\n",
        "importances_permutation = pd.Series(permu_imp['importances_mean'], index=features)\n",
        "importances_permutation.sort_values(ascending=True)[-10:].plot.barh()\n",
        "plt.title('Permutation Importance on Out-of-Sample Set')\n",
        "plt.xlabel('change in log likelihood');"
      ],
      "metadata": {
        "id": "sMtiqxZSbH9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}