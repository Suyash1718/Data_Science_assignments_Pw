{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Can we use Bagging for regression problems ?\n",
        "\n",
        "\n",
        "Yes, you can absolutely use bagging for regression problems, and it's a common and effective approach to improve the stability and accuracy of regression models. Bagging, or bootstrap aggregating, works by training multiple regression models on different subsets of the training data, and then averaging their predictions to produce a final, more robust prediction."
      ],
      "metadata": {
        "id": "mrS32dn0Xy-q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3biWlLGDru32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between multiple model training and single model training ?\n",
        "\n",
        "\n",
        "\n",
        "Single Model Training:\n",
        "Focus:\n",
        "Develops a single model to learn patterns and make predictions from data.\n",
        "Process:\n",
        "The model is trained using a specific algorithm and parameters, and its performance is evaluated based on its ability to generalize to unseen data.\n",
        "Advantages:\n",
        "Simpler to implement and manage.\n",
        "Can be faster to train than multiple models.\n",
        "May be more interpretable, depending on the model type.\n",
        "Disadvantages:\n",
        "Performance can be limited by the model's architecture and training data.\n",
        "Can be prone to overfitting or underfitting.\n",
        "\n",
        "\n",
        "Multiple Model Training (Ensemble Methods):\n",
        "Focus:\n",
        "Uses multiple models, often trained independently, and combines their predictions.\n",
        "Process:\n",
        "Training: Each model is trained using different algorithms, parameters, or data subsets.\n",
        "Combination: The predictions from the individual models are combined using a voting scheme, averaging, or other methods to produce a final prediction.\n",
        "Advantages:\n",
        "Can achieve higher accuracy and robustness than single models.\n",
        "Can be more resilient to errors or biases in individual models.\n",
        "Disadvantages:\n",
        "More complex to implement and manage.\n",
        "Can be slower to train and predict than single models.\n",
        "May be less interpretable than single models.\n",
        "Examples of Ensemble Methods:\n",
        "Bagging:\n",
        "Training multiple models on different subsets of the training data and averaging their predictions (e.g., Random Forest).\n",
        "Boosting:\n",
        "Training models sequentially, where each model focuses on correcting the errors of the previous models (e.g., XGBoost, LightGBM).\n",
        "Stacking:\n",
        "Training multiple models and then using another model (the meta-model) to combine their predictions.\n",
        "In summary: Choose single model training for simplicity and speed, and ensemble methods when higher accuracy and robustness are crucial, even at the cost of complexity and training time."
      ],
      "metadata": {
        "id": "vstK3295YA5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "\n",
        "\n",
        "In Random Forest, feature randomness, also known as feature bagging or random subspace method, means that at each node of a decision tree, the algorithm randomly selects a subset of features to consider for splitting, rather than using all available features. This randomness helps to decorrelate the trees, leading to a more robust and accurate model with better generalization capabilities."
      ],
      "metadata": {
        "id": "_XAVVfRHYW9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is OOB (Out-of-Bag) Score ?\n",
        "\n",
        "\n",
        "\n",
        "Out-of-bag (OOB) score, also known as OOB error, is a method to assess the prediction error of machine learning models, particularly those using bagging techniques like Random Forests, without needing a separate validation or test set. It leverages the \"out-of-bag\" samples (data points not used in training a particular model) to estimate the model's performance."
      ],
      "metadata": {
        "id": "toVX6yvTYjzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  How can you measure the importance of features in a Random Forest model ?\n",
        "\n",
        "\n",
        "\n",
        "In Random Forest models, feature importance is typically measured using the mean decrease in impurity (or Gini importance), which quantifies how much a feature reduces the impurity of a node when used for splitting, averaged across all trees in the forest."
      ],
      "metadata": {
        "id": "Y1XPkV12Y0-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "\n",
        "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction."
      ],
      "metadata": {
        "id": "x6bjrCE3ZCUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you evaluate a Bagging Classifier’s performance ?\n",
        "\n",
        "\n",
        "\n",
        "To evaluate a Bagging Classifier's performance, you should train the model, make predictions on a test set, and then use metrics like accuracy, precision, recall, and F1-score to assess its performance."
      ],
      "metadata": {
        "id": "jBR0SDFyZKnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  How does a Bagging Regressor work ?\n",
        "\n",
        "\n",
        "A Bagging Regressor works by training multiple base regression models on different random subsets of the training data (bootstrap samples), and then aggregating their predictions (typically by averaging) to produce a final prediction."
      ],
      "metadata": {
        "id": "fqc7tmsEZcu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is the main advantage of ensemble techniques ?\n",
        "\n",
        "The main advantage of ensemble techniques in machine learning is improved predictive performance and generalization by combining multiple models, often resulting in higher accuracy and robustness than individual models.\n"
      ],
      "metadata": {
        "id": "0hp3kFn5ZnrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What is the main challenge of ensemble methods ?\n",
        "\n",
        "\n",
        "The main challenge of ensemble methods lies in their increased computational complexity and the potential for overfitting, requiring careful selection and tuning of both base models and the meta-model."
      ],
      "metadata": {
        "id": "IBDZY1vQZx_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Explain the key idea behind ensemble techniques.\n",
        "\n",
        "\n",
        "Ensemble techniques in machine learning combine multiple models to achieve better performance than any single model, leveraging the collective strengths of diverse models to reduce errors and improve accuracy.\n",
        "\n",
        "\n",
        "\n",
        "The Core Idea:\n",
        "Ensemble learning is based on the principle that combining the predictions of multiple models can overcome the limitations of individual models, leading to a more robust and accurate final prediction.\n",
        "\n",
        "How it Works:\n",
        "Multiple Models: Ensemble methods involve training multiple models (often \"weak learners\") on the same dataset or different subsets of it.\n",
        "Combining Predictions: The predictions from these individual models are then combined using a specific strategy, such as averaging (for regression), majority voting (for classification), or a more sophisticated method like weighted averaging.\n",
        "\n",
        "Improved Performance:\n",
        "The resulting ensemble model often exhibits better generalization performance, meaning it performs well on unseen data, compared to any of the individual models alone.\n",
        "\n",
        "Types of Ensemble Techniques:\n",
        "Bagging: (Bootstrap Aggregating) trains multiple models on different subsets of the training data, and then combines their predictions.\n",
        "\n",
        "Boosting: trains models sequentially, with each model focusing on the errors made by previous models.\n",
        "\n",
        "Stacking: trains multiple models and then uses their predictions as input to a meta-model (a model that learns how to combine the predictions of the base models).\n",
        "\n",
        "Benefits of Ensemble Learning:\n",
        "Improved Accuracy: By combining multiple perspectives, ensemble models often achieve higher accuracy than single models.\n",
        "\n",
        "Reduced Overfitting: Ensemble methods can help to mitigate overfitting by averaging out the predictions of multiple models.\n",
        "\n",
        "Robustness: Ensemble models are often more robust to noisy data and outliers because they rely on the collective wisdom of multiple models.\n",
        "\n",
        "Examples of Ensemble Algorithms:\n",
        "Random Forest (a bagging method)\n",
        "Gradient Boosting (a boosting method)\n",
        "XGBoost (a boosting method)\n",
        "Stacking (a stacking method)"
      ],
      "metadata": {
        "id": "Jwx44T4zZ9v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is a Random Forest Classifier?\n",
        "\n",
        "\n",
        " A Random Forest classifier is a machine learning algorithm that uses an ensemble of decision trees to make predictions, combining the results of multiple trees to improve accuracy and reduce overfitting."
      ],
      "metadata": {
        "id": "TchFdGVJabo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are the main types of ensemble techniques ?\n",
        "\n",
        "\n",
        "\n",
        "Ensemble learning techniques. Perhaps three of the most popular ensemble learning techniques are bagging, boosting, and stacking. In fact, these together exemplify distinctions between sequential, parallel, homogenous, and heterogenous types of ensemble methods."
      ],
      "metadata": {
        "id": "oaXe9M_4aqLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is ensemble learning in machine learning ?\n",
        "\n",
        "\n",
        "Ensemble learning in machine learning combines multiple models to improve predictive performance, leveraging the strengths of diverse models to achieve better accuracy and robustness than a single model."
      ],
      "metadata": {
        "id": "5IKXnUNgazaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should we avoid using ensemble methods ?\n",
        "\n",
        "\n",
        "Avoid ensemble methods when computational resources are severely limited, data is insufficient or highly correlated, or interpretability is paramount, as they can be computationally intensive, complex, and potentially difficult to explain."
      ],
      "metadata": {
        "id": "bSf1jukKbRvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. How does Bagging help in reducing overfitting ?\n",
        "\n",
        "\n",
        "\n",
        "Bagging attempts to reduce the chance overfitting complex models. It trains a large number of “strong” learners in parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to “smooth out” their predictions."
      ],
      "metadata": {
        "id": "-9hafqguba_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Why is Random Forest better than a single Decision Tree ?\n",
        "\n",
        "\n",
        "Random Forest is generally considered better than a single decision tree because it mitigates overfitting, improves accuracy and generalization by combining multiple trees, and provides insights into feature importance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sgxPlLJlbmZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the role of bootstrap sampling in Bagging ?\n",
        "\n",
        "In bagging (Bootstrap Aggregating), bootstrap sampling creates diverse subsets of the training data by randomly sampling with replacement, allowing the same data point to appear multiple times in a subset, which is then used to train multiple models, which are then aggregated to improve prediction accuracy and stability."
      ],
      "metadata": {
        "id": "nQCLlrxdb3t9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What are some real-world applications of ensemble techniques ?\n",
        "\n",
        "\n",
        "\n",
        "Ensemble techniques, which combine multiple models to improve prediction accuracy, find applications in diverse fields like fraud detection, medical diagnostics, stock market prediction, and customer behavior analysis."
      ],
      "metadata": {
        "id": "T4WpoA7NcDqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "\n",
        "Bagging and boosting are both ensemble learning methods that combine multiple models to improve accuracy and stability. The main difference between the two is how the models are trained."
      ],
      "metadata": {
        "id": "EqUiHUt4cSFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#practical**"
      ],
      "metadata": {
        "id": "UVPHutbHcd2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#21.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the data\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Parameters\n",
        "n_models = 100\n",
        "random_states = [i for i in range(n_models)]\n",
        "\n",
        "\n",
        "# Helper function for bootstrapping\n",
        "def bootstrapping(X, y):\n",
        "    n_samples = X.shape[0]\n",
        "    idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
        "    return X[idxs], y[idxs]\n",
        "\n",
        "\n",
        "    # Helper function for bagging prediction\n",
        "def predict(X, models):\n",
        "    predictions = np.array([model.predict(X) for model in models])\n",
        "    predictions = stats.mode(predictions)[0]\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "    # Create a list to store all the tree models\n",
        "tree_models = []\n",
        "\n",
        "# Iteratively train decision trees on bootstrapped samples\n",
        "for i in range(n_models):\n",
        "    X_, y_ = bootstrapping(X_train, y_train)\n",
        "    tree = DecisionTreeClassifier(max_depth=2, random_state=random_states[i])\n",
        "    tree.fit(X_, y_)\n",
        "    tree_models.append(tree)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = predict(X_test, tree_models)\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joacr3TIclCC",
        "outputId": "5a921a0e-2eed-4e3b-e435-24945c2129eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "df.head().T\n",
        "\n",
        "\n",
        "\n",
        "df[\"target\"].value_counts()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree model\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "\n",
        "# Evaluate the single Decision Tree\n",
        "accuracy_single = accuracy_score(y_test, y_pred_single)\n",
        "print(f\"Accuracy of Single Decision Tree: {accuracy_single:.2f}\")\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees as base learners\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Base model is a decision tree\n",
        "    n_estimators=10,                          # Train 10 decision trees\n",
        "    random_state=42,\n",
        "    bootstrap=True                            # Use bootstrap sampling\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Bagging Classifier\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5cU-gBddbSv",
        "outputId": "884bfcb5-27b0-4f1d-b5ed-c91cd7cf6786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 0.96\n",
            "Accuracy of Bagging Classifier: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\n",
        "# importing the libraries\n",
        "from functools import reduce\n",
        "# linear algebra\n",
        "import numpy as np\n",
        "# data processing, CSV file I/O\n",
        "import pandas as pd\n",
        "# data visualization library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "import time\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import RFECV\n",
        "dataset = pd.read_csv(\"../data/data.csv\")\n",
        "\n",
        "\n",
        "\n",
        "def get_xy(data:pd.DataFrame,list_drp:list):\n",
        "        \"\"\"\n",
        "        set the x and y column\n",
        "\n",
        "        args:\n",
        "            data(pd.DataFrame): the dataFrame which we are extracting the x and y\n",
        "\n",
        "        returns:\n",
        "            y and X in form of pandas series\n",
        "\n",
        "        \"\"\"\n",
        "        y = data.diagnosis # M or B\n",
        "        X = data.drop(list_drp,axis = 1 )\n",
        "        return y,X\n",
        "d_list = ['Unnamed: 32','id','diagnosis']\n",
        "y,x = get_xy(dataset,d_list)\n",
        "x.head()"
      ],
      "metadata": {
        "id": "wM484ywYcvuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #24.Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "X_grid = np.arange(min(X),max(X),0.01)\n",
        "X_grid = X_grid.reshape(len(X_grid),1)\n",
        "\n",
        "plt.scatter(X,y, color='blue') #plotting real points\n",
        "plt.plot(X_grid, regressor.predict(X_grid),color='green') #plotting for predict points\n",
        "\n",
        "plt.title(\"Random Forest Regression Results\")\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aOHes5d9epxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "\n",
        "\n",
        "\n",
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RANDOM_STATE = 123\n",
        "\n",
        "# Generate a binary classification dataset.\n",
        "X, y = make_classification(\n",
        "    n_samples=500,\n",
        "    n_features=25,\n",
        "    n_clusters_per_class=1,\n",
        "    n_informative=15,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
        "# support for parallelized ensembles but is necessary for tracking the OOB\n",
        "# error trajectory during training.\n",
        "ensemble_clfs = [\n",
        "    (\n",
        "        \"RandomForestClassifier, max_features='sqrt'\",\n",
        "        RandomForestClassifier(\n",
        "            warm_start=True,\n",
        "            oob_score=True,\n",
        "            max_features=\"sqrt\",\n",
        "            random_state=RANDOM_STATE,\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"RandomForestClassifier, max_features='log2'\",\n",
        "        RandomForestClassifier(\n",
        "            warm_start=True,\n",
        "            max_features=\"log2\",\n",
        "            oob_score=True,\n",
        "            random_state=RANDOM_STATE,\n",
        "        ),\n",
        "    ),\n",
        "    (\n",
        "        \"RandomForestClassifier, max_features=None\",\n",
        "        RandomForestClassifier(\n",
        "            warm_start=True,\n",
        "            max_features=None,\n",
        "            oob_score=True,\n",
        "            random_state=RANDOM_STATE,\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
        "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
        "\n",
        "# Range of `n_estimators` values to explore.\n",
        "min_estimators = 15\n",
        "max_estimators = 150\n",
        "\n",
        "for label, clf in ensemble_clfs:\n",
        "    for i in range(min_estimators, max_estimators + 1, 5):\n",
        "        clf.set_params(n_estimators=i)\n",
        "        clf.fit(X, y)\n",
        "\n",
        "        # Record the OOB error for each `n_estimators=i` setting.\n",
        "        oob_error = 1 - clf.oob_score_\n",
        "        error_rate[label].append((i, oob_error))\n",
        "\n",
        "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
        "for label, clf_err in error_rate.items():\n",
        "    xs, ys = zip(*clf_err)\n",
        "    plt.plot(xs, ys, label=label)\n",
        "\n",
        "plt.xlim(min_estimators, max_estimators)\n",
        "plt.xlabel(\"n_estimators\")\n",
        "plt.ylabel(\"OOB error rate\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P3nQbe6PiJmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "df = pd.DataFrame(X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "df.head().T\n",
        "\n",
        "df[\"target\"].value_counts()\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree model\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "y_pred_single = single_tree.predict(X_test)\n",
        "\n",
        "# Evaluate the single Decision Tree\n",
        "accuracy_single = accuracy_score(y_test, y_pred_single)\n",
        "print(f\"Accuracy of Single Decision Tree: {accuracy_single:.2f}\")\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees as base learners\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Base model is a decision tree\n",
        "    n_estimators=10,                          # Train 10 decision trees\n",
        "    random_state=42,\n",
        "    bootstrap=True                            # Use bootstrap sampling\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test)\n",
        "\n",
        "# Evaluate the Bagging Classifier\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bagging:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq9Cqi5mifMP",
        "outputId": "85ba7db3-81c7-4f11-e576-4a54b34ca262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 0.96\n",
            "Accuracy of Bagging Classifier: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27 Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create and prepare dataset\n",
        "dataset_dict = {\n",
        "    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast',\n",
        "                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',\n",
        "                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',\n",
        "                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n",
        "    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n",
        "                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n",
        "                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n",
        "    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n",
        "                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n",
        "                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n",
        "    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n",
        "             True, False, True, True, False, False, True, False, True, True, False,\n",
        "             True, False, False, True, False, False],\n",
        "    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',\n",
        "             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',\n",
        "             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n",
        "}\n",
        "\n",
        "# Prepare data\n",
        "df = pd.DataFrame(dataset_dict)\n",
        "df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\n",
        "df['Wind'] = df['Wind'].astype(int)\n",
        "df['Play'] = (df['Play'] == 'Yes').astype(int)\n",
        "\n",
        "# Rearrange columns\n",
        "column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']\n",
        "\n",
        "df = df[column_order]\n",
        "\n",
        "# Prepare features and target\n",
        "X,y = df.drop('Play', axis=1), df['Play']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyWW8NVEjZzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29.Train a Random Forest Regressor and analyze feature importance scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "importance = clf.feature_importances_\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(range(X.shape[1]), importance)\n",
        "plt.xticks(range(X.shape[1]), iris.feature_names, rotation=90)\n",
        "plt.title(\"Feature Importance in Random Forest\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q1pCB1mzkdHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n"
      ],
      "metadata": {
        "id": "03M21zx1tjZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#32.Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "X, y = make_regression(n_samples=100, n_features=4,\n",
        "                       n_informative=2, n_targets=1,\n",
        "                       random_state=0, shuffle=False)\n",
        "regr = BaggingRegressor(estimator=SVR(),\n",
        "                        n_estimators=10, random_state=0).fit(X, y)\n",
        "regr.predict([[0, 0, 0, 0]])"
      ],
      "metadata": {
        "id": "il2fiQwxuN6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier  # For creating decision tree classifiers\n",
        "from sklearn.svm import SVC  # For creating Support Vector Machine classifiers\n",
        "\n",
        "# Import model evaluation and ensemble method\n",
        "from sklearn.metrics import accuracy_score  # For evaluating model accuracy\n",
        "from sklearn.ensemble import BaggingClassifier  # For ensemble learning using bagging\n",
        "\n",
        "# Import utility functions for data handling and preparation\n",
        "from sklearn.model_selection import train_test_split  # For splitting the dataset into train and test sets\n",
        "from sklearn.datasets import make_classification  # For generating a synthetic classification dataset\n",
        "import pandas as pd  # For handling data frames\n",
        "import numpy as np  # For numerical operations\n",
        "\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_informative=3)\n",
        "\n",
        "# Splitting the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bag.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the labels of the test data\n",
        "y_pred = bag.predict(X_test)\n",
        "\n",
        "# Printing the accuracy of the Bagging Classifier\n",
        "print(\"Bagging Classifier accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Printing the shape of the samples and features used by the first estimator\n",
        "# to illustrate how the Bagging Classifier diversifies training across different learners\n",
        "print(\"Shape of the samples used by the first estimator:\", bag.estimators_samples_[0].shape)\n",
        "print(\"Shape of the features used by the first estimator:\", bag.estimators_features_[0].shape)"
      ],
      "metadata": {
        "id": "1LN_1blyutK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#35.Train a Random Forest Classifier and visualize the confusion matrix\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#Fit the model\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fig(X,y)\n",
        "#Generate predictions with the model using our X values\n",
        "y_pred = logreg.predict(X)\n",
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(y, y_pred)\n",
        "print(cf_matrix)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "\n",
        "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\n",
        "            fmt='.2%', cmap='Blues')\n",
        "\n",
        "labels = [‘True Neg’,’False Pos’,’False Neg’,’True Pos’]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt=‘’, cmap='Blues')\n",
        "\n",
        "group_names = [‘True Neg’,’False Pos’,’False Neg’,’True Pos’]\n",
        "group_counts = [“{0:0.0f}”.format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [“{0:.2%}”.format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f”{v1}\\n{v2}\\n{v3}” for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt=‘’, cmap='Blues')"
      ],
      "metadata": {
        "id": "0qasM-QKwAkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#37. Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=3,\n",
        "    n_redundant=0,\n",
        "    n_repeated=0,\n",
        "    n_classes=2,\n",
        "    random_state=0,\n",
        "    shuffle=False,\n",
        ")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
        "forest = RandomForestClassifier(random_state=0)\n",
        "forest.fit(X_train, y_train)\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "start_time = time.time()\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances using MDI\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "fig.tight_layout()37."
      ],
      "metadata": {
        "id": "CFEqBeazwnaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Example data\n",
        "y_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 0])\n",
        "y_scores = np.array([0.1, 0.7, 0.8, 0.3, 0.9, 0.6, 0.2, 0.4, 0.7, 0.5])\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XzVZIUBNx2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#42 Train a Bagging Classifier and evaluate its performance using cross-validatio\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X.shape, y.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=0)\n",
        "\n",
        "X_train.shape, y_train.shape\n",
        "X_test.shape, y_test.shape\n",
        "\n",
        "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "clf.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KXXdZquVyQQJ",
        "outputId": "b0d47bb9-95fb-4c81-e293-ec6bebc8cb82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#43.Train a Random Forest Classifier and plot the Precision-Recall curv\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "# Calculate AUC\n",
        "auc_score = auc(recall, precision)\n",
        "\n",
        "print(f\"Area Under the Precision-Recall Curve: {auc_score:.4f}\")\n",
        "\n",
        "# Plot Precision-Recall curve with AUC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AUC = {auc_score:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve with AUC')\n",
        "plt.legend(loc='lower left')\n",
        "plt.fill_between(recall, precision, alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v9oMjb2Cyym8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "# Importing necessary libraries\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the base classifier (in this case, a decision tree)\n",
        "\n",
        "base_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Initialize the BaggingClassifier\n",
        "\n",
        "# You can specify the number of base estimators (n_estimators) and other parameters\n",
        "\n",
        "bagging_classifier = BaggingClassifier(base_estimator=base_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the BaggingClassifier\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Xzw33kWOzOuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}